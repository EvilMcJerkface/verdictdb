{
    "docs": [
        {
            "location": "/",
            "text": "VerdictDB Documentation\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nVerdictDB is a thin, platform-independent, interactive analytics library that works on top of your existing (or backend) database system (e.g., MySQL, PostgreSQL, Redshift, etc.). For platform-independence, VerdictDB makes all communications with the backend database in SQL. For interactive querying, VerdictDB has the ability to properly adjust the output values even when only a fraction of the data is processed. The initial outputs are already pretty accurate, and they can become more accurate (naturally) as more data is processed. These partial (and incremental) data processing are performed automatically by VerdictDB using query rewriting. The connections to VerdictDB can be made in Java.\n\n\nNote: Python interface will be soon added.\n\n\nContents\n\u00b6\n\n\n\n\nGetting Started\n\n\nQuickstart\n\n\nInstall / Download\n\n\nConnecting to Databases\n\n\nScrambling\n\n\nInteractive Querying\n\n\n\n\n\n\nHow it works\n\n\nArchitecture\n\n\nQuery Processing\n\n\n\n\n\n\nTutorial\n\n\nSetting up TPC-H data\n\n\n\n\n\n\n\n\nLicense\n\u00b6\n\n\nVerdictDB is under the Apache License, thus is completely free for both commercial and non-commercial purposes.",
            "title": "Home"
        },
        {
            "location": "/#verdictdb-documentation",
            "text": "",
            "title": "VerdictDB Documentation"
        },
        {
            "location": "/#introduction",
            "text": "VerdictDB is a thin, platform-independent, interactive analytics library that works on top of your existing (or backend) database system (e.g., MySQL, PostgreSQL, Redshift, etc.). For platform-independence, VerdictDB makes all communications with the backend database in SQL. For interactive querying, VerdictDB has the ability to properly adjust the output values even when only a fraction of the data is processed. The initial outputs are already pretty accurate, and they can become more accurate (naturally) as more data is processed. These partial (and incremental) data processing are performed automatically by VerdictDB using query rewriting. The connections to VerdictDB can be made in Java.  Note: Python interface will be soon added.",
            "title": "Introduction"
        },
        {
            "location": "/#contents",
            "text": "Getting Started  Quickstart  Install / Download  Connecting to Databases  Scrambling  Interactive Querying    How it works  Architecture  Query Processing    Tutorial  Setting up TPC-H data",
            "title": "Contents"
        },
        {
            "location": "/#license",
            "text": "VerdictDB is under the Apache License, thus is completely free for both commercial and non-commercial purposes.",
            "title": "License"
        },
        {
            "location": "/getting_started/quickstart/",
            "text": "Quickstart Guide\n\u00b6\n\n\nWe will install VerdictDB, create a connection, and issue a simple query to VerdictDB. In this Quickstart Guide, we will use an MySQL database for VerdictDB's backend database. See \nHow to Connect\n for the examples of connecting to other databases.\n\n\nInstall\n\u00b6\n\n\nCreate a \nMaven\n project and\nplace the following dependency in your pom.xml.\n\n<dependency>\n    <groupId>org.verdictdb</groupId>\n    <artifactId>verdictdb-core</artifactId>\n    <version>0.5.0-alpha</version>\n</dependency>\n\n\n\nTo use MySQL, add the following entry as well:\n\n<dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n    <version>5.1.46</version>\n</dependency>\n\n\n\nInsert Data\n\u00b6\n\n\nWe will first generate small data to play with.\n\n\n// Suppose username is root and password is rootpassword.\n\n\nConnection\n \nmysqlConn\n \n=\n \nDriverManager\n.\ngetConnection\n(\n\"jdbc:mysql://localhost\"\n,\n \n\"root\"\n,\n \n\"rootpassword\"\n);\n\n\nStatement\n \nstmt\n \n=\n \nmysqlConn\n.\ncreateStatement\n();\n\n\nstmt\n.\nexecute\n(\n\"create schema myschema\"\n);\n\n\nstmt\n.\nexecute\n(\n\"create table myschema.sales (\"\n \n+\n\n             \n\"  product   varchar(100),\"\n \n+\n\n             \n\"  price     double)\"\n);\n\n\n\n// insert 1000 rows\n\n\nList\n<\nString\n>\n \nproductList\n \n=\n \nArrays\n.\nasList\n(\n\"milk\"\n,\n \n\"egg\"\n,\n \n\"juice\"\n);\n\n\nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n<\n \n1000\n;\n \ni\n++)\n \n{\n\n  \nint\n \nrandInt\n \n=\n \nThreadLocalRandom\n.\ncurrent\n().\nnextInt\n(\n0\n,\n \n3\n)\n\n  \nString\n \nproduct\n \n=\n \nproductList\n.\nget\n(\nrandInt\n);\n\n  \ndouble\n \nprice\n \n=\n \n(\nrandInt\n+\n2\n)\n \n*\n \n10\n \n+\n \nThreadLocalRandom\n.\ncurrent\n().\nnextInt\n(\n0\n,\n \n10\n);\n\n  \nstmt\n.\nexecute\n(\nString\n.\nformat\n(\n\n      \n\"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\"\n,\n\n      \nproduct\n,\n \nprice\n));\n\n\n}\n\n\n\n\n\nTest VerdictDB\n\u00b6\n\n\nCreate a JDBC connection to VerdictDB.\n\n\nConnection\n \nverdict\n \n=\n \nDriverManager\n.\ngetConnection\n(\n\"jdbc:verdict:mysql://localhost\"\n,\n \n\"root\"\n,\n \n\"rootpassword\"\n);\n\n\nStatement\n \nvstmt\n \n=\n \nverdict\n.\ncreateStatement\n();\n\n\n\n\n\nCreate a special table called a \"scramble\", which is the replica of the original table with extra information VerdictDB uses for speeding up query processing.\n\n\nvstmt\n.\nexecute\n(\n\"create scramble myschema.sales_scrambled from myschema.sales\"\n);\n\n\n\n\n\nRun just a regular query to the original table.\n\n\nResultSet\n \nrs\n \n=\n \nverdictStmt\n.\nexecuteQuery\n(\n\n    \n\"select product, avg(price) \"\n+\n\n    \n\"from myschema.sales_scrambled \"\n \n+\n\n    \n\"group by product \"\n \n+\n\n    \n\"order by product\"\n);\n\n\n\n\n\nInternally, VerdictDB rewrites the above query to use the scramble. It is equivalent to explicitly specifying the scramble in the from clause of the above query.\n\n\nComplete Example Java File\n\u00b6\n\n\n(Yongjoo: update this according to the above code)\n\n\nimport\n \njava.sql.*\n;\n\n\nimport\n \njava.util.ArrayList\n;\n\n\nimport\n \njava.util.Arrays\n;\n\n\nimport\n \njava.util.List\n;\n\n\n\npublic\n \nclass\n \nFirstVerdictDBExample\n \n{\n\n\n\n  \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n \nargs\n[])\n \nthrows\n \nSQLException\n \n{\n\n    \n// Suppose username is root and password is rootpassword.\n\n    \nConnection\n \nmysqlConn\n \n=\n \nDriverManager\n.\ngetConnection\n(\n\"jdbc:mysql://localhost\"\n,\n \n\"root\"\n,\n \n\"rootpassword\"\n);\n\n    \nStatement\n \nstmt\n \n=\n \nmysqlConn\n.\ncreateStatement\n();\n\n    \nstmt\n.\nexecute\n(\n\"create schema myschema\"\n);\n\n    \nstmt\n.\nexecute\n(\n\"create table myschema.sales (\"\n \n+\n\n                 \n\"  product   varchar(100),\"\n \n+\n\n                 \n\"  price     double)\"\n);\n\n\n    \n// insert 1000 rows\n\n    \nList\n<\nString\n>\n \nproductList\n \n=\n \nArrays\n.\nasList\n(\n\"milk\"\n,\n \n\"egg\"\n,\n \n\"juice\"\n);\n\n    \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n<\n \n1000\n;\n \ni\n++)\n \n{\n\n      \nint\n \nrandInt\n \n=\n \nThreadLocalRandom\n.\ncurrent\n().\nnextInt\n(\n0\n,\n \n3\n)\n\n      \nString\n \nproduct\n \n=\n \nproductList\n.\nget\n(\nrandInt\n);\n\n      \ndouble\n \nprice\n \n=\n \n(\nrandInt\n+\n2\n)\n \n*\n \n10\n \n+\n \nThreadLocalRandom\n.\ncurrent\n().\nnextInt\n(\n0\n,\n \n10\n);\n\n      \nstmt\n.\nexecute\n(\nString\n.\nformat\n(\n\n          \n\"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\"\n,\n\n          \nproduct\n,\n \nprice\n));\n\n    \n}\n\n\n    \nConnection\n \nverdict\n \n=\n \nDriverManager\n.\ngetConnection\n(\n\"jdbc:verdict:mysql://localhost\"\n,\n \n\"root\"\n,\n \n\"rootpassword\"\n);\n\n    \nStatement\n \nvstmt\n \n=\n \nverdict\n.\ncreateStatement\n();\n\n\n    \n// Use CREATE SCRAMBLE syntax to create scrambled tables.\n\n    \nvstmt\n.\nexecute\n(\n\"create scramble myschema.sales_scrambled from myschema.sales\"\n);\n\n\n    \nResultSet\n \nrs\n \n=\n \nverdictStmt\n.\nexecuteQuery\n(\n\n        \n\"select product, avg(price) \"\n+\n\n        \n\"from myschema.sales_scrambled \"\n \n+\n\n        \n\"group by product \"\n \n+\n\n        \n\"order by product\"\n);\n\n\n    \n// Do something after getting the results.\n\n  \n}\n\n\n}",
            "title": "Quickstart"
        },
        {
            "location": "/getting_started/quickstart/#quickstart-guide",
            "text": "We will install VerdictDB, create a connection, and issue a simple query to VerdictDB. In this Quickstart Guide, we will use an MySQL database for VerdictDB's backend database. See  How to Connect  for the examples of connecting to other databases.",
            "title": "Quickstart Guide"
        },
        {
            "location": "/getting_started/quickstart/#install",
            "text": "Create a  Maven  project and\nplace the following dependency in your pom.xml. <dependency>\n    <groupId>org.verdictdb</groupId>\n    <artifactId>verdictdb-core</artifactId>\n    <version>0.5.0-alpha</version>\n</dependency>  To use MySQL, add the following entry as well: <dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n    <version>5.1.46</version>\n</dependency>",
            "title": "Install"
        },
        {
            "location": "/getting_started/quickstart/#insert-data",
            "text": "We will first generate small data to play with.  // Suppose username is root and password is rootpassword.  Connection   mysqlConn   =   DriverManager . getConnection ( \"jdbc:mysql://localhost\" ,   \"root\" ,   \"rootpassword\" );  Statement   stmt   =   mysqlConn . createStatement ();  stmt . execute ( \"create schema myschema\" );  stmt . execute ( \"create table myschema.sales (\"   + \n              \"  product   varchar(100),\"   + \n              \"  price     double)\" );  // insert 1000 rows  List < String >   productList   =   Arrays . asList ( \"milk\" ,   \"egg\" ,   \"juice\" );  for   ( int   i   =   0 ;   i   <   1000 ;   i ++)   { \n   int   randInt   =   ThreadLocalRandom . current (). nextInt ( 0 ,   3 ) \n   String   product   =   productList . get ( randInt ); \n   double   price   =   ( randInt + 2 )   *   10   +   ThreadLocalRandom . current (). nextInt ( 0 ,   10 ); \n   stmt . execute ( String . format ( \n       \"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\" , \n       product ,   price ));  }",
            "title": "Insert Data"
        },
        {
            "location": "/getting_started/quickstart/#test-verdictdb",
            "text": "Create a JDBC connection to VerdictDB.  Connection   verdict   =   DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" ,   \"root\" ,   \"rootpassword\" );  Statement   vstmt   =   verdict . createStatement ();   Create a special table called a \"scramble\", which is the replica of the original table with extra information VerdictDB uses for speeding up query processing.  vstmt . execute ( \"create scramble myschema.sales_scrambled from myschema.sales\" );   Run just a regular query to the original table.  ResultSet   rs   =   verdictStmt . executeQuery ( \n     \"select product, avg(price) \" + \n     \"from myschema.sales_scrambled \"   + \n     \"group by product \"   + \n     \"order by product\" );   Internally, VerdictDB rewrites the above query to use the scramble. It is equivalent to explicitly specifying the scramble in the from clause of the above query.",
            "title": "Test VerdictDB"
        },
        {
            "location": "/getting_started/quickstart/#complete-example-java-file",
            "text": "(Yongjoo: update this according to the above code)  import   java.sql.* ;  import   java.util.ArrayList ;  import   java.util.Arrays ;  import   java.util.List ;  public   class   FirstVerdictDBExample   { \n\n\n   public   static   void   main ( String   args [])   throws   SQLException   { \n     // Suppose username is root and password is rootpassword. \n     Connection   mysqlConn   =   DriverManager . getConnection ( \"jdbc:mysql://localhost\" ,   \"root\" ,   \"rootpassword\" ); \n     Statement   stmt   =   mysqlConn . createStatement (); \n     stmt . execute ( \"create schema myschema\" ); \n     stmt . execute ( \"create table myschema.sales (\"   + \n                  \"  product   varchar(100),\"   + \n                  \"  price     double)\" ); \n\n     // insert 1000 rows \n     List < String >   productList   =   Arrays . asList ( \"milk\" ,   \"egg\" ,   \"juice\" ); \n     for   ( int   i   =   0 ;   i   <   1000 ;   i ++)   { \n       int   randInt   =   ThreadLocalRandom . current (). nextInt ( 0 ,   3 ) \n       String   product   =   productList . get ( randInt ); \n       double   price   =   ( randInt + 2 )   *   10   +   ThreadLocalRandom . current (). nextInt ( 0 ,   10 ); \n       stmt . execute ( String . format ( \n           \"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\" , \n           product ,   price )); \n     } \n\n     Connection   verdict   =   DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" ,   \"root\" ,   \"rootpassword\" ); \n     Statement   vstmt   =   verdict . createStatement (); \n\n     // Use CREATE SCRAMBLE syntax to create scrambled tables. \n     vstmt . execute ( \"create scramble myschema.sales_scrambled from myschema.sales\" ); \n\n     ResultSet   rs   =   verdictStmt . executeQuery ( \n         \"select product, avg(price) \" + \n         \"from myschema.sales_scrambled \"   + \n         \"group by product \"   + \n         \"order by product\" ); \n\n     // Do something after getting the results. \n   }  }",
            "title": "Complete Example Java File"
        },
        {
            "location": "/getting_started/install/",
            "text": "Download/Install\n\u00b6\n\n\nUse a different interface depending on your language preference (Java or Python).\n\n\nNote: Python interface is currently in preparation.\n\n\nJava\n\u00b6\n\n\nOne of the following three methods can be used:\n\n\n\n\nMaven\n\n\nUsing a pre-compiled jar\n\n\nBuild yourself\n\n\n\n\nMaven\n\u00b6\n\n\nIf you use \nApache Maven\n for your project's dependency management, adding the following dependency entry to your \npom.xml\n is all you need to do to use VerdictDB.\n\n\n<dependency>\n    <groupId>org.verdictdb</groupId>\n    <artifactId>verdictdb-core</artifactId>\n    <version>0.5.0-alpha</version>\n</dependency>\n\n\n\n\nDownload a Pre-compiled Jar\n\u00b6\n\n\nYou only need a single jar file. This jar file is compiled with JDK8.\n\n\nDownload\n: \nverdictdb-core-0.5.0-alpha-jar-with-dependencies.jar\n\n\nBuild Yourself\n\u00b6\n\n\n\n\nClone\n from our \nGithub public repository\n. Use command\n    \ngit clone https://github.com/mozafari/verdictdb.git\n\n\n\n\n\nChange\n directory to the directory to the repository you have cloned. Use command,\n    \ncd verdictdb\n\n\n\n\n\n\n\nBuild\n the jar file by maven. Use command\n    \nmvn -DskipTests -DtestPhase=false -DpackagePhase=true clean package\n\n\n    Check the \ntarget\n directory for the created jar files.\n\n\n\n\n\n\nPython\n\u00b6\n\n\nThis is in preparation.",
            "title": "Download/Install"
        },
        {
            "location": "/getting_started/install/#downloadinstall",
            "text": "Use a different interface depending on your language preference (Java or Python).  Note: Python interface is currently in preparation.",
            "title": "Download/Install"
        },
        {
            "location": "/getting_started/install/#java",
            "text": "One of the following three methods can be used:   Maven  Using a pre-compiled jar  Build yourself",
            "title": "Java"
        },
        {
            "location": "/getting_started/install/#maven",
            "text": "If you use  Apache Maven  for your project's dependency management, adding the following dependency entry to your  pom.xml  is all you need to do to use VerdictDB.  <dependency>\n    <groupId>org.verdictdb</groupId>\n    <artifactId>verdictdb-core</artifactId>\n    <version>0.5.0-alpha</version>\n</dependency>",
            "title": "Maven"
        },
        {
            "location": "/getting_started/install/#download-a-pre-compiled-jar",
            "text": "You only need a single jar file. This jar file is compiled with JDK8.  Download :  verdictdb-core-0.5.0-alpha-jar-with-dependencies.jar",
            "title": "Download a Pre-compiled Jar"
        },
        {
            "location": "/getting_started/install/#build-yourself",
            "text": "Clone  from our  Github public repository . Use command\n     git clone https://github.com/mozafari/verdictdb.git   Change  directory to the directory to the repository you have cloned. Use command,\n     cd verdictdb    Build  the jar file by maven. Use command\n     mvn -DskipTests -DtestPhase=false -DpackagePhase=true clean package \n    Check the  target  directory for the created jar files.",
            "title": "Build Yourself"
        },
        {
            "location": "/getting_started/install/#python",
            "text": "This is in preparation.",
            "title": "Python"
        },
        {
            "location": "/getting_started/connection/",
            "text": "Connecting to Data Sources\n\u00b6\n\n\nSupported Databases\n\u00b6\n\n\n\n\nMySQL 5.5 or later\n\n\nPostgreSQL 10 or later\n\n\nAmazon Redshift\n\n\nImpala 2.5 (CDH 5.7) or later\n\n\nSpark 2.0 or later\n\n\n\n\nThe following databases will be supported soon:\n\n\n\n\nHive\n\n\nOracle\n\n\nSQL Server\n\n\nPresto\n\n\n\n\nConnection Options\n\u00b6\n\n\n\n\nStandard JDBC Interface\n: One can issue queries to and retrieve the results from VerdictDB using the standard JDBC interface. This approach is applicable for all databases that support the JDBC interface. For VerdictDB to retrieve data from the backend database, VerdictDB requires the connection information to the backend database. This connection information can be specified by either of the two ways, as follows.\n\n\nJDBC string\n: Passing a modified JDBC string to the standard Java DriverManager. See database-specific examples below.\n\n\nJDBC connection\n: Passing an already established JDBC connection (to the backend DB) to VerdictDB. See database-specific examples below.\n\n\n\n\n\n\nVerdictContext\n: One can also connect to VerdictDB directly using its own interface called VerdictContext. An instance of VerdictContext can be created either using the JDBC connection information or using an instance of SparkSession. The query results returned from VerdictContext use ?? for convenient operations.\n\n\n\n\nMySQL\n\u00b6\n\n\nJDBC string\n\u00b6\n\n\nSee that the \nverdict\n keyword is inserted to the regular JDBC connection string for the MySQL connection.\n\n\nString\n \nconnectionString\n \n=\n\n    \nString\n.\nformat\n(\n\"jdbc:verdict:mysql://%s:%d/%s\"\n,\n\n        \nMYSQL_HOST\n,\n \nMYSQL_PORT\n,\n \nMYSQL_DATABASE\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\nconnectionString\n,\n \nMYSQL_UESR\n,\n \nMYSQL_PASSWORD\n);\n\n\n\n\n\nJDBC connection\n\u00b6\n\n\n// create MySQL JDBC connection\n\n\nString\n \nmysqlConnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:mysql://%s/%s\"\n,\n \nMYSQL_HOST\n,\n \nMYSQL_DATABASE\n);\n\n\nConnection\n \nmysqlConn\n \n=\n \nDriverManager\n.\ngetConnection\n(\nmysqlConnectionString\n,\n \nMYSQL_USER\n,\n \nMYSQL_PASSWORD\n);\n\n\n// modify MySQL JDBC connection URL and create VerdictConnection\n\n\nStringBuilder\n \njdbcUrl\n \n=\n \nnew\n \nStringBuilder\n(\npostgresConn\n.\ngetMetaData\n().\ngetURL\n());\n\n\njdbcUrl\n.\ninsert\n(\n5\n,\n \n\"verdict:\"\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\njdbcUrl\n.\ntoString\n(),\n \nMYSQL_USER\n,\n \nMYSQL_PASSWORD\n);\n\n\n\n\n\nPostgreSQL\n\u00b6\n\n\nJDBC string\n\u00b6\n\n\n// use JDBC connection URL as connection string\n\n\nString\n \nconnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:verdict:postgresql://%s:%d/%s?user=%s&password=%s\"\n,\n \nPOSTGRES_HOST\n,\n \nPOSTGRES_PORT\n,\n \nPOSTGRES_DATABASE\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\nconnectionString\n,\n \nPOSTGRES_USER\n,\n \nPOSTGRES_PASSWORD\n);\n\n\n\n\n\nJDBC connection\n\u00b6\n\n\n// create PostgreSQL JDBC connection\n\n\nString\n \npostgresConnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:postgresql://%s/%s\"\n,\n \nPOSTGRES_HOST\n,\n \nPOSTGRES_DATABASE\n);\n\n\nConnection\n \npostgresConn\n \n=\n \nDriverManager\n.\ngetConnection\n(\npostgresConnectionString\n,\n \nPOSTGRES_USER\n,\n \nPOSTGRES_PASSWORD\n);\n\n\n// modify PostgreSQL JDBC connection URL and create VerdictConnection\n\n\nStringBuilder\n \njdbcUrl\n \n=\n \nnew\n \nStringBuilder\n(\npostgresConn\n.\ngetMetaData\n().\ngetURL\n());\n\n\njdbcUrl\n.\ninsert\n(\n5\n,\n \n\"verdict:\"\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\njdbcUrl\n.\ntoString\n(),\n \nPOSTGRES_USER\n,\n \nPOSTGRES_PASSWORD\n);\n\n\n\n\n\nRedshift\n\u00b6\n\n\nJDBC string\n\u00b6\n\n\n// use JDBC connection URL as connection string\n\n\nString\n \nconnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:verdict:redshift://%s:%d/%s\"\n,\n \nREDSHIFT_HOST\n,\n \nREDSHIFT_PORT\n,\n \nREDSHIFT_DATABASE\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\nconnectionString\n,\n \nREDSHIFT_USER\n,\n \nREDSHIFT_PASSWORD\n);\n\n\n\n\n\nJDBC connection\n\u00b6\n\n\n// create RedShift JDBC connection\n\n\nString\n \nredshiftConnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:redshift://%s/%s\"\n,\n \nREDSHIFT_HOST\n,\n \nREDSHIFT_DATABASE\n);\n\n\nConnection\n \nredshiftConn\n \n=\n \nDriverManager\n.\ngetConnection\n(\nredshiftConnectionString\n,\n \nREDSHIFT_USER\n,\n \nREDSHIFT_PASSWORD\n);\n\n\n// modify RedShift JDBC connection URL and create VerdictConnection\n\n\nStringBuilder\n \njdbcUrl\n \n=\n \nnew\n \nStringBuilder\n(\nredshiftConn\n.\ngetMetaData\n().\ngetURL\n());\n\n\njdbcUrl\n.\ninsert\n(\n5\n,\n \n\"verdict:\"\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\njdbcUrl\n.\ntoString\n(),\n \nREDSHIFT_USER\n,\n \nREDSHIFT_PASSWORD\n);\n\n\n\n\n\nCloudera Impala\n\u00b6\n\n\nJDBC string\n\u00b6\n\n\n// use JDBC connection URL as connection string\n\n\nString\n \nimpalaConnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:verdict:impala://%s:%d/%s\"\n,\n \nIMPALA_HOST\n,\n \nIMPALA_PORT\n,\n \nIMPALA_DATABASE\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\nimpalaConnectionString\n,\n \nIMPALA_USER\n,\n \nIMPALA_PASSWORD\n);\n\n\n\n\n\nJDBC connection\n\u00b6\n\n\n// create Impala JDBC connection\n\n\nString\n \nimpalaConnectionString\n \n=\n\n        \nString\n.\nformat\n(\n\"jdbc:impala://%s/%s\"\n,\n \nIMPALA_HOST\n,\n \nIMPALA_DATABASE\n);\n\n\nConnection\n \nimpalaConn\n \n=\n \nDriverManager\n.\ngetConnection\n(\nimpalaConnectionString\n,\n \nIMPALA_USER\n,\n \nIMPALA_PASSWORD\n);\n\n\n// modify Impala JDBC connection URL and create VerdictConnection\n\n\nStringBuilder\n \njdbcUrl\n \n=\n \nnew\n \nStringBuilder\n(\nimpalaConn\n.\ngetMetaData\n().\ngetURL\n());\n\n\njdbcUrl\n.\ninsert\n(\n5\n,\n \n\"verdict:\"\n);\n\n\nConnection\n \nvc\n \n=\n \nDriverManager\n.\ngetConnection\n(\njdbcUrl\n.\ntoString\n(),\n \nIMPALA_USER\n,\n \nIMPALA_PASSWORD\n);\n\n\n\n\n\nApache Spark\n\u00b6\n\n\nSpark Session\n\u00b6\n\n\n(Yongjoo will write this later)",
            "title": "Connecting to Data Sources"
        },
        {
            "location": "/getting_started/connection/#connecting-to-data-sources",
            "text": "",
            "title": "Connecting to Data Sources"
        },
        {
            "location": "/getting_started/connection/#supported-databases",
            "text": "MySQL 5.5 or later  PostgreSQL 10 or later  Amazon Redshift  Impala 2.5 (CDH 5.7) or later  Spark 2.0 or later   The following databases will be supported soon:   Hive  Oracle  SQL Server  Presto",
            "title": "Supported Databases"
        },
        {
            "location": "/getting_started/connection/#connection-options",
            "text": "Standard JDBC Interface : One can issue queries to and retrieve the results from VerdictDB using the standard JDBC interface. This approach is applicable for all databases that support the JDBC interface. For VerdictDB to retrieve data from the backend database, VerdictDB requires the connection information to the backend database. This connection information can be specified by either of the two ways, as follows.  JDBC string : Passing a modified JDBC string to the standard Java DriverManager. See database-specific examples below.  JDBC connection : Passing an already established JDBC connection (to the backend DB) to VerdictDB. See database-specific examples below.    VerdictContext : One can also connect to VerdictDB directly using its own interface called VerdictContext. An instance of VerdictContext can be created either using the JDBC connection information or using an instance of SparkSession. The query results returned from VerdictContext use ?? for convenient operations.",
            "title": "Connection Options"
        },
        {
            "location": "/getting_started/connection/#mysql",
            "text": "",
            "title": "MySQL"
        },
        {
            "location": "/getting_started/connection/#jdbc-string",
            "text": "See that the  verdict  keyword is inserted to the regular JDBC connection string for the MySQL connection.  String   connectionString   = \n     String . format ( \"jdbc:verdict:mysql://%s:%d/%s\" , \n         MYSQL_HOST ,   MYSQL_PORT ,   MYSQL_DATABASE );  Connection   vc   =   DriverManager . getConnection ( connectionString ,   MYSQL_UESR ,   MYSQL_PASSWORD );",
            "title": "JDBC string"
        },
        {
            "location": "/getting_started/connection/#jdbc-connection",
            "text": "// create MySQL JDBC connection  String   mysqlConnectionString   = \n         String . format ( \"jdbc:mysql://%s/%s\" ,   MYSQL_HOST ,   MYSQL_DATABASE );  Connection   mysqlConn   =   DriverManager . getConnection ( mysqlConnectionString ,   MYSQL_USER ,   MYSQL_PASSWORD );  // modify MySQL JDBC connection URL and create VerdictConnection  StringBuilder   jdbcUrl   =   new   StringBuilder ( postgresConn . getMetaData (). getURL ());  jdbcUrl . insert ( 5 ,   \"verdict:\" );  Connection   vc   =   DriverManager . getConnection ( jdbcUrl . toString (),   MYSQL_USER ,   MYSQL_PASSWORD );",
            "title": "JDBC connection"
        },
        {
            "location": "/getting_started/connection/#postgresql",
            "text": "",
            "title": "PostgreSQL"
        },
        {
            "location": "/getting_started/connection/#jdbc-string_1",
            "text": "// use JDBC connection URL as connection string  String   connectionString   = \n         String . format ( \"jdbc:verdict:postgresql://%s:%d/%s?user=%s&password=%s\" ,   POSTGRES_HOST ,   POSTGRES_PORT ,   POSTGRES_DATABASE );  Connection   vc   =   DriverManager . getConnection ( connectionString ,   POSTGRES_USER ,   POSTGRES_PASSWORD );",
            "title": "JDBC string"
        },
        {
            "location": "/getting_started/connection/#jdbc-connection_1",
            "text": "// create PostgreSQL JDBC connection  String   postgresConnectionString   = \n         String . format ( \"jdbc:postgresql://%s/%s\" ,   POSTGRES_HOST ,   POSTGRES_DATABASE );  Connection   postgresConn   =   DriverManager . getConnection ( postgresConnectionString ,   POSTGRES_USER ,   POSTGRES_PASSWORD );  // modify PostgreSQL JDBC connection URL and create VerdictConnection  StringBuilder   jdbcUrl   =   new   StringBuilder ( postgresConn . getMetaData (). getURL ());  jdbcUrl . insert ( 5 ,   \"verdict:\" );  Connection   vc   =   DriverManager . getConnection ( jdbcUrl . toString (),   POSTGRES_USER ,   POSTGRES_PASSWORD );",
            "title": "JDBC connection"
        },
        {
            "location": "/getting_started/connection/#redshift",
            "text": "",
            "title": "Redshift"
        },
        {
            "location": "/getting_started/connection/#jdbc-string_2",
            "text": "// use JDBC connection URL as connection string  String   connectionString   = \n         String . format ( \"jdbc:verdict:redshift://%s:%d/%s\" ,   REDSHIFT_HOST ,   REDSHIFT_PORT ,   REDSHIFT_DATABASE );  Connection   vc   =   DriverManager . getConnection ( connectionString ,   REDSHIFT_USER ,   REDSHIFT_PASSWORD );",
            "title": "JDBC string"
        },
        {
            "location": "/getting_started/connection/#jdbc-connection_2",
            "text": "// create RedShift JDBC connection  String   redshiftConnectionString   = \n         String . format ( \"jdbc:redshift://%s/%s\" ,   REDSHIFT_HOST ,   REDSHIFT_DATABASE );  Connection   redshiftConn   =   DriverManager . getConnection ( redshiftConnectionString ,   REDSHIFT_USER ,   REDSHIFT_PASSWORD );  // modify RedShift JDBC connection URL and create VerdictConnection  StringBuilder   jdbcUrl   =   new   StringBuilder ( redshiftConn . getMetaData (). getURL ());  jdbcUrl . insert ( 5 ,   \"verdict:\" );  Connection   vc   =   DriverManager . getConnection ( jdbcUrl . toString (),   REDSHIFT_USER ,   REDSHIFT_PASSWORD );",
            "title": "JDBC connection"
        },
        {
            "location": "/getting_started/connection/#cloudera-impala",
            "text": "",
            "title": "Cloudera Impala"
        },
        {
            "location": "/getting_started/connection/#jdbc-string_3",
            "text": "// use JDBC connection URL as connection string  String   impalaConnectionString   = \n         String . format ( \"jdbc:verdict:impala://%s:%d/%s\" ,   IMPALA_HOST ,   IMPALA_PORT ,   IMPALA_DATABASE );  Connection   vc   =   DriverManager . getConnection ( impalaConnectionString ,   IMPALA_USER ,   IMPALA_PASSWORD );",
            "title": "JDBC string"
        },
        {
            "location": "/getting_started/connection/#jdbc-connection_3",
            "text": "// create Impala JDBC connection  String   impalaConnectionString   = \n         String . format ( \"jdbc:impala://%s/%s\" ,   IMPALA_HOST ,   IMPALA_DATABASE );  Connection   impalaConn   =   DriverManager . getConnection ( impalaConnectionString ,   IMPALA_USER ,   IMPALA_PASSWORD );  // modify Impala JDBC connection URL and create VerdictConnection  StringBuilder   jdbcUrl   =   new   StringBuilder ( impalaConn . getMetaData (). getURL ());  jdbcUrl . insert ( 5 ,   \"verdict:\" );  Connection   vc   =   DriverManager . getConnection ( jdbcUrl . toString (),   IMPALA_USER ,   IMPALA_PASSWORD );",
            "title": "JDBC connection"
        },
        {
            "location": "/getting_started/connection/#apache-spark",
            "text": "",
            "title": "Apache Spark"
        },
        {
            "location": "/getting_started/connection/#spark-session",
            "text": "(Yongjoo will write this later)",
            "title": "Spark Session"
        },
        {
            "location": "/getting_started/scrambling/",
            "text": "Creating Scrambles\n\u00b6\n\n\nSyntax\n\u00b6\n\n\nCREATE\n \nSCRAMBLE\n \nnewSchema\n.\nnewTable\n \nFROM\n \noriginalSchema\n.\noriginalTable\n;\n\n\n\n\n\nNote:\n\n\n\n\n`newSchema\n may be identical to \noriginalSchema\n.\n\n\nnewTable\n must be different from \noriginalTable\n.\n\n\nThe user requires the write privilege for the \nnewSchema\n schema and the read privilege for the \noriginalSchema.originalTable\n.\n\n\nThe schema and table names can be quoted either using the double-quote (\") or the backtick (`).\n\n\nThe schema names, table names, column names, etc. in the queries issued by VerdictDB to the backend database are always quoted.\n\n\n\n\nWhat is a Scramble?\n\u00b6\n\n\nFor VerdictDB's interactive query processing, a special table called a \nscramble\n must be created for an original table. The queries including scrambles are automatically rewritten by VerdictDB in a way to enable interactive querying. The queries including the original table(s) are also first rewritten to use its/their corresponding scrambles; then, VerdictDB applies to the same mechanism to enable interactive querying for those queries. If no scrambles have been created for a table, no query rewritting is performed.\n\n\nEvery time a scramble is created, VerdictDB stores the information in its own metadata schema (\nverdictdbmetadata\n by default). The relationship between the original tables and scrambles are recognized using this metadata.",
            "title": "Creating Scrambles"
        },
        {
            "location": "/getting_started/scrambling/#creating-scrambles",
            "text": "",
            "title": "Creating Scrambles"
        },
        {
            "location": "/getting_started/scrambling/#syntax",
            "text": "CREATE   SCRAMBLE   newSchema . newTable   FROM   originalSchema . originalTable ;   Note:   `newSchema  may be identical to  originalSchema .  newTable  must be different from  originalTable .  The user requires the write privilege for the  newSchema  schema and the read privilege for the  originalSchema.originalTable .  The schema and table names can be quoted either using the double-quote (\") or the backtick (`).  The schema names, table names, column names, etc. in the queries issued by VerdictDB to the backend database are always quoted.",
            "title": "Syntax"
        },
        {
            "location": "/getting_started/scrambling/#what-is-a-scramble",
            "text": "For VerdictDB's interactive query processing, a special table called a  scramble  must be created for an original table. The queries including scrambles are automatically rewritten by VerdictDB in a way to enable interactive querying. The queries including the original table(s) are also first rewritten to use its/their corresponding scrambles; then, VerdictDB applies to the same mechanism to enable interactive querying for those queries. If no scrambles have been created for a table, no query rewritting is performed.  Every time a scramble is created, VerdictDB stores the information in its own metadata schema ( verdictdbmetadata  by default). The relationship between the original tables and scrambles are recognized using this metadata.",
            "title": "What is a Scramble?"
        },
        {
            "location": "/getting_started/querying/",
            "text": "Interactive Querying\n\u00b6\n\n\nVerdictDB can give interactive answers for aggregate queries, which include \navg\n, \nsum\n, \ncount\n, \nmin\n, and \nmax\n. The SQL query syntax is almost identical to the standard.\n\n\nSyntax\n\u00b6\n\n\nSELECT\n \n[\nselect_item\n,\n \n]\n \naggregate_expr\n \n[,\n \naggregate_expr\n]\n \n[,\n \n...]\n\n\nFROM\n \ntable_source\n\n\n[\nWHERE\n \npredicate\n]\n\n\n[\nGROUP\n \nBY\n \ngroup_expr\n]\n\n\n[\nORDER\n \nBY\n \nordering_expr\n]\n\n\n[\nLIMIT\n \nnumber\n];\n\n\n\nselect_item\n \n:\n=\n \nregular_expr\n \n[\nAS\n \nalias\n];\n\n\n\ntable_source\n \n:\n=\n \nbase_table\n\n              \n|\n \nbase_table\n,\n \ntable_source\n\n              \n|\n \nbase_table\n \n[\nINNER\n \n|\n \nLEFT\n \n|\n \nRIGHT\n]\n \nJOIN\n \ntable_source\n \nON\n \ncondition\n\n              \n|\n \nselect_query\n;\n\n\n\ngroup_expr\n \n:\n=\n \nregular_expr\n \n|\n \nalias\n;\n\n\n\naggregate_expr\n \n:\n=\n \navg\n(\nregular_expr\n)\n\n                \n|\n \nsum\n(\nregular_expr\n)\n\n                \n|\n \ncount\n(\nregular_expr\n)\n\n                \n|\n \nmin\n(\nregular_expr\n)\n\n                \n|\n \nmax\n(\nregular_expr\n);\n\n\n\nregular_expr\n \n:\n=\n \nunary_func\n(\nregular_expr\n)\n\n              \n|\n \nbinary_func\n(\nregular_expr\n,\n \nregular_expr\n)\n\n              \n|\n \nregular_expr\n \nop\n \nregular_expr\n;\n\n\n\nop\n \n:\n=\n \n+\n \n|\n \n-\n \n|\n \n*\n \n|\n \n/\n \n|\n \nand\n \n|\n \nor\n;\n\n\n\n\n\nHow does VerdictDB compute them?\n\u00b6\n\n\nVerdictDB applies different rules for different types of aggregate functions as follows. VerdictDB relies on the state-of-the-art techniques available in the literature.\n\n\nAVG, SUM, COUNT\n\u00b6\n\n\nVerdictDB's answers are always \nunbiased estimators\n of the true answers. For instance, if only 10% of the data (that amounts to the 10% uniform random sample of the data) is processed, the unbiased estimator for the count function is 10 times the answer computed on the 10% of the data. This logic becomes more complex as unbiased samples (within scrambles) are used for different types of aggregate functions. VerdictDB performs these computations (and proper scaling) all automatically.\n\n\nMIN, MAX\n\u00b6\n\n\nVerdictDB's answers to min and max functions are the min and max of the data that has been processed so far. For example, if 10% of the data was processed, then VerdictDB outputs min or max among those 10% data. Of course, the answers become more accurate as more data is processed and become exact when 100% data is processed. One possible concern is that the answers based on partial data may not be very accurate especially when a tiny fraction (e.g., 0.1%) of the data has been processed. To overcome this, VerdictDB processes outliers first. As a result, even the answers at the early stages are highly accurate.\n\n\nCOUNT-DISTINCT\n\u00b6\n\n\nThis is in preparation.",
            "title": "Interactive Querying"
        },
        {
            "location": "/getting_started/querying/#interactive-querying",
            "text": "VerdictDB can give interactive answers for aggregate queries, which include  avg ,  sum ,  count ,  min , and  max . The SQL query syntax is almost identical to the standard.",
            "title": "Interactive Querying"
        },
        {
            "location": "/getting_started/querying/#syntax",
            "text": "SELECT   [ select_item ,   ]   aggregate_expr   [,   aggregate_expr ]   [,   ...]  FROM   table_source  [ WHERE   predicate ]  [ GROUP   BY   group_expr ]  [ ORDER   BY   ordering_expr ]  [ LIMIT   number ];  select_item   : =   regular_expr   [ AS   alias ];  table_source   : =   base_table \n               |   base_table ,   table_source \n               |   base_table   [ INNER   |   LEFT   |   RIGHT ]   JOIN   table_source   ON   condition \n               |   select_query ;  group_expr   : =   regular_expr   |   alias ;  aggregate_expr   : =   avg ( regular_expr ) \n                 |   sum ( regular_expr ) \n                 |   count ( regular_expr ) \n                 |   min ( regular_expr ) \n                 |   max ( regular_expr );  regular_expr   : =   unary_func ( regular_expr ) \n               |   binary_func ( regular_expr ,   regular_expr ) \n               |   regular_expr   op   regular_expr ;  op   : =   +   |   -   |   *   |   /   |   and   |   or ;",
            "title": "Syntax"
        },
        {
            "location": "/getting_started/querying/#how-does-verdictdb-compute-them",
            "text": "VerdictDB applies different rules for different types of aggregate functions as follows. VerdictDB relies on the state-of-the-art techniques available in the literature.",
            "title": "How does VerdictDB compute them?"
        },
        {
            "location": "/getting_started/querying/#avg-sum-count",
            "text": "VerdictDB's answers are always  unbiased estimators  of the true answers. For instance, if only 10% of the data (that amounts to the 10% uniform random sample of the data) is processed, the unbiased estimator for the count function is 10 times the answer computed on the 10% of the data. This logic becomes more complex as unbiased samples (within scrambles) are used for different types of aggregate functions. VerdictDB performs these computations (and proper scaling) all automatically.",
            "title": "AVG, SUM, COUNT"
        },
        {
            "location": "/getting_started/querying/#min-max",
            "text": "VerdictDB's answers to min and max functions are the min and max of the data that has been processed so far. For example, if 10% of the data was processed, then VerdictDB outputs min or max among those 10% data. Of course, the answers become more accurate as more data is processed and become exact when 100% data is processed. One possible concern is that the answers based on partial data may not be very accurate especially when a tiny fraction (e.g., 0.1%) of the data has been processed. To overcome this, VerdictDB processes outliers first. As a result, even the answers at the early stages are highly accurate.",
            "title": "MIN, MAX"
        },
        {
            "location": "/getting_started/querying/#count-distinct",
            "text": "This is in preparation.",
            "title": "COUNT-DISTINCT"
        },
        {
            "location": "/how_it_works/architecture/",
            "text": "Architecture\n\u00b6",
            "title": "Architecture"
        },
        {
            "location": "/how_it_works/architecture/#architecture",
            "text": "",
            "title": "Architecture"
        },
        {
            "location": "/how_it_works/query_processing/",
            "text": "Query Processing\n\u00b6",
            "title": "Query Processing"
        },
        {
            "location": "/how_it_works/query_processing/#query-processing",
            "text": "",
            "title": "Query Processing"
        },
        {
            "location": "/tutorial/tpch/",
            "text": "TPC-H Data Setup\n\u00b6\n\n\nThis is a step-by-step guide for setting up TPC-H data in different databases. This guide will use 1GB data. This guide assumes you have basic knowledge about issuing commands in a terminal application.\n\n\nDownload Data\n\u00b6\n\n\nGo to your work directory (say \n/home/username/workspace\n) and download the data:\n\n\ncd\n /home/username/workspace\ncurl http://dbgroup-internal.eecs.umich.edu/projects/verdictdb/tpch1g.zip -o tpch1g.zip\n\n\n\n\nUnzip the downloaded file:\n\n\nunzip tpch1g.zip\n\n\n\n\nIt will create a new directory named \ntpch1g\n under your work directory. The directory contains 8 sub-directories for each of 8 tables.\n\n\nMySQL\n\u00b6\n\n\nCreate tables\n\u00b6\n\n\nConnect to your MySQL database.\n\n\nmysql -uroot\n\n\n\n\nCreate a schema for test.\n\n\nmysql> create database tpch1g\n;\n\n\n\n\n\nCreate empty tables; simply copy and paste the following table definition statements into the MySQL shell. We will import the data later into these tables.\n\n\n-- nation\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\nnation\n \n(\n\n  \n`\nn_nationkey\n`\n  \nINT\n,\n\n  \n`\nn_name\n`\n       \nCHAR\n(\n25\n),\n\n  \n`\nn_regionkey\n`\n  \nINT\n,\n\n  \n`\nn_comment\n`\n    \nVARCHAR\n(\n152\n),\n\n  \n`\nn_dummy\n`\n      \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\nn_nationkey\n`\n));\n\n\n\n-- region\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\nregion\n \n(\n\n  \n`\nr_regionkey\n`\n  \nINT\n,\n\n  \n`\nr_name\n`\n       \nCHAR\n(\n25\n),\n\n  \n`\nr_comment\n`\n    \nVARCHAR\n(\n152\n),\n\n  \n`\nr_dummy\n`\n      \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\nr_regionkey\n`\n));\n\n\n\n-- supplier\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\nsupplier\n \n(\n\n  \n`\ns_suppkey\n`\n     \nINT\n,\n\n  \n`\ns_name\n`\n        \nCHAR\n(\n25\n),\n\n  \n`\ns_address\n`\n     \nVARCHAR\n(\n40\n),\n\n  \n`\ns_nationkey\n`\n   \nINT\n,\n\n  \n`\ns_phone\n`\n       \nCHAR\n(\n15\n),\n\n  \n`\ns_acctbal\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\ns_comment\n`\n     \nVARCHAR\n(\n101\n),\n\n  \n`\ns_dummy\n`\n \nvarchar\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\ns_suppkey\n`\n));\n\n\n\n-- customer\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\ncustomer\n \n(\n\n  \n`\nc_custkey\n`\n     \nINT\n,\n\n  \n`\nc_name\n`\n        \nVARCHAR\n(\n25\n),\n\n  \n`\nc_address\n`\n     \nVARCHAR\n(\n40\n),\n\n  \n`\nc_nationkey\n`\n   \nINT\n,\n\n  \n`\nc_phone\n`\n       \nCHAR\n(\n15\n),\n\n  \n`\nc_acctbal\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nc_mktsegment\n`\n  \nCHAR\n(\n10\n),\n\n  \n`\nc_comment\n`\n     \nVARCHAR\n(\n117\n),\n\n  \n`\nc_dummy\n`\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\nc_custkey\n`\n));\n\n\n\n-- part\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\npart\n \n(\n\n  \n`\np_partkey\n`\n     \nINT\n,\n\n  \n`\np_name\n`\n        \nVARCHAR\n(\n55\n),\n\n  \n`\np_mfgr\n`\n        \nCHAR\n(\n25\n),\n\n  \n`\np_brand\n`\n       \nCHAR\n(\n10\n),\n\n  \n`\np_type\n`\n        \nVARCHAR\n(\n25\n),\n\n  \n`\np_size\n`\n        \nINT\n,\n\n  \n`\np_container\n`\n   \nCHAR\n(\n10\n),\n\n  \n`\np_retailprice\n`\n \nDECIMAL\n(\n15\n,\n2\n)\n \n,\n\n  \n`\np_comment\n`\n     \nVARCHAR\n(\n23\n)\n \n,\n\n  \n`\np_dummy\n`\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\np_partkey\n`\n));\n\n\n\n-- partsupp\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\npartsupp\n \n(\n\n  \n`\nps_partkey\n`\n     \nINT\n,\n\n  \n`\nps_suppkey\n`\n     \nINT\n,\n\n  \n`\nps_availqty\n`\n    \nINT\n,\n\n  \n`\nps_supplycost\n`\n  \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nps_comment\n`\n     \nVARCHAR\n(\n199\n),\n\n  \n`\nps_dummy\n`\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\nps_partkey\n`\n));\n\n\n\n-- orders\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\norders\n \n(\n\n  \n`\no_orderkey\n`\n       \nINT\n,\n\n  \n`\no_custkey\n`\n        \nINT\n,\n\n  \n`\no_orderstatus\n`\n    \nCHAR\n(\n1\n),\n\n  \n`\no_totalprice\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\no_orderdate\n`\n      \nDATE\n,\n\n  \n`\no_orderpriority\n`\n  \nCHAR\n(\n15\n),\n\n  \n`\no_clerk\n`\n          \nCHAR\n(\n15\n),\n\n  \n`\no_shippriority\n`\n   \nINT\n,\n\n  \n`\no_comment\n`\n        \nVARCHAR\n(\n79\n),\n\n  \n`\no_dummy\n`\n          \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n`\no_orderkey\n`\n));\n\n\n\n-- lineitem\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ntpch1g\n.\nlineitem\n \n(\n\n  \n`\nl_orderkey\n`\n    \nINT\n,\n\n  \n`\nl_partkey\n`\n     \nINT\n,\n\n  \n`\nl_suppkey\n`\n     \nINT\n,\n\n  \n`\nl_linenumber\n`\n  \nINT\n,\n\n  \n`\nl_quantity\n`\n    \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_extendedprice\n`\n  \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_discount\n`\n    \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_tax\n`\n         \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_returnflag\n`\n  \nCHAR\n(\n1\n),\n\n  \n`\nl_linestatus\n`\n  \nCHAR\n(\n1\n),\n\n  \n`\nl_shipdate\n`\n    \nDATE\n,\n\n  \n`\nl_commitdate\n`\n  \nDATE\n,\n\n  \n`\nl_receiptdate\n`\n \nDATE\n,\n\n  \n`\nl_shipinstruct\n`\n \nCHAR\n(\n25\n),\n\n  \n`\nl_shipmode\n`\n    \nCHAR\n(\n10\n),\n\n  \n`\nl_comment\n`\n     \nVARCHAR\n(\n44\n),\n\n  \n`\nl_dummy\n`\n       \nVARCHAR\n(\n10\n));\n\n\n\n\n\nImport Data\n\u00b6\n\n\nSuppose your work directory is \n/home/username/workspace\n and the tpch1g data is stored in \n/home/username/workspace/tpch1g\n. Then, issue the following commands in the MySQL shell to load the data.\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/region/region.tbl'\n     \nINTO\n \nTABLE\n \nregion\n     \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/nation/nation.tbl'\n     \nINTO\n \nTABLE\n \nnation\n     \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/customer/customer.tbl'\n \nINTO\n \nTABLE\n \ncustomer\n   \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/supplier/supplier.tbl'\n \nINTO\n \nTABLE\n \nsupplier\n   \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/part/part.tbl'\n         \nINTO\n \nTABLE\n \npart\n       \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/partsupp/partsupp.tbl'\n \nINTO\n \nTABLE\n \npartsupp\n   \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/orders/orders.tbl'\n     \nINTO\n \nTABLE\n \norders\n     \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\nLOAD\n \nDATA\n \nLOCAL\n \nINFILE\n \n'/home/username/workspace/tpch1g/lineitem/lineitem.tbl'\n \nINTO\n \nTABLE\n \nlineitem\n   \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n;\n\n\n\n\n\nPostgreSQL\n\u00b6\n\n\nCreate tables\n\u00b6\n\n\nConnect to your Postgresql database.\n\n\npsql\n\n\n\n\nCreate a schema for testing.\n\n\npostgres=# create schema \"tpch1g\";\n\n\n\n\nCreate empty tables; simply copy and paste the following table definition statements into the PostgreSQL prompt. We will import the data later into these tables.\n\n\n-- nation\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"nation\"\n \n(\n\n  \n\"n_nationkey\"\n  \nINT\n,\n\n  \n\"n_name\"\n       \nCHAR\n(\n25\n),\n\n  \n\"n_regionkey\"\n  \nINT\n,\n\n  \n\"n_comment\"\n    \nVARCHAR\n(\n152\n),\n\n  \n\"n_dummy\"\n      \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"n_nationkey\"\n));\n\n\n\n-- region\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"region\"\n \n(\n\n  \n\"r_regionkey\"\n  \nINT\n,\n\n  \n\"r_name\"\n       \nCHAR\n(\n25\n),\n\n  \n\"r_comment\"\n    \nVARCHAR\n(\n152\n),\n\n  \n\"r_dummy\"\n      \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"r_regionkey\"\n));\n\n\n\n-- supplier\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"supplier\"\n \n(\n\n  \n\"s_suppkey\"\n     \nINT\n,\n\n  \n\"s_name\"\n        \nCHAR\n(\n25\n),\n\n  \n\"s_address\"\n     \nVARCHAR\n(\n40\n),\n\n  \n\"s_nationkey\"\n   \nINT\n,\n\n  \n\"s_phone\"\n       \nCHAR\n(\n15\n),\n\n  \n\"s_acctbal\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"s_comment\"\n     \nVARCHAR\n(\n101\n),\n\n  \n\"s_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"s_suppkey\"\n));\n\n\n\n-- customer\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"customer\"\n \n(\n\n  \n\"c_custkey\"\n     \nINT\n,\n\n  \n\"c_name\"\n        \nVARCHAR\n(\n25\n),\n\n  \n\"c_address\"\n     \nVARCHAR\n(\n40\n),\n\n  \n\"c_nationkey\"\n   \nINT\n,\n\n  \n\"c_phone\"\n       \nCHAR\n(\n15\n),\n\n  \n\"c_acctbal\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"c_mktsegment\"\n  \nCHAR\n(\n10\n),\n\n  \n\"c_comment\"\n     \nVARCHAR\n(\n117\n),\n\n  \n\"c_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"c_custkey\"\n));\n\n\n\n-- part\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"part\"\n \n(\n\n  \n\"p_partkey\"\n     \nINT\n,\n\n  \n\"p_name\"\n        \nVARCHAR\n(\n55\n),\n\n  \n\"p_mfgr\"\n        \nCHAR\n(\n25\n),\n\n  \n\"p_brand\"\n       \nCHAR\n(\n10\n),\n\n  \n\"p_type\"\n        \nVARCHAR\n(\n25\n),\n\n  \n\"p_size\"\n        \nINT\n,\n\n  \n\"p_container\"\n   \nCHAR\n(\n10\n),\n\n  \n\"p_retailprice\"\n \nDECIMAL\n(\n15\n,\n2\n)\n \n,\n\n  \n\"p_comment\"\n     \nVARCHAR\n(\n23\n)\n \n,\n\n  \n\"p_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"p_partkey\"\n));\n\n\n\n-- partsupp\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"partsupp\"\n \n(\n\n  \n\"ps_partkey\"\n     \nINT\n,\n\n  \n\"ps_suppkey\"\n     \nINT\n,\n\n  \n\"ps_availqty\"\n    \nINT\n,\n\n  \n\"ps_supplycost\"\n  \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"ps_comment\"\n     \nVARCHAR\n(\n199\n),\n\n  \n\"ps_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"ps_partkey\"\n));\n\n\n\n-- orders\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"orders\"\n \n(\n\n  \n\"o_orderkey\"\n       \nINT\n,\n\n  \n\"o_custkey\"\n        \nINT\n,\n\n  \n\"o_orderstatus\"\n    \nCHAR\n(\n1\n),\n\n  \n\"o_totalprice\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"o_orderdate\"\n      \nDATE\n,\n\n  \n\"o_orderpriority\"\n  \nCHAR\n(\n15\n),\n\n  \n\"o_clerk\"\n          \nCHAR\n(\n15\n),\n\n  \n\"o_shippriority\"\n   \nINT\n,\n\n  \n\"o_comment\"\n        \nVARCHAR\n(\n79\n),\n\n  \n\"o_dummy\"\n          \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"o_orderkey\"\n));\n\n\n\n-- lineitem\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"lineitem\"\n(\n\n  \n\"l_orderkey\"\n          \nINT\n,\n\n  \n\"l_partkey\"\n           \nINT\n,\n\n  \n\"l_suppkey\"\n           \nINT\n,\n\n  \n\"l_linenumber\"\n        \nINT\n,\n\n  \n\"l_quantity\"\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_extendedprice\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_discount\"\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_tax\"\n               \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_returnflag\"\n        \nCHAR\n(\n1\n),\n\n  \n\"l_linestatus\"\n        \nCHAR\n(\n1\n),\n\n  \n\"l_shipdate\"\n          \nDATE\n,\n\n  \n\"l_commitdate\"\n        \nDATE\n,\n\n  \n\"l_receiptdate\"\n       \nDATE\n,\n\n  \n\"l_shipinstruct\"\n      \nCHAR\n(\n25\n),\n\n  \n\"l_shipmode\"\n          \nCHAR\n(\n10\n),\n\n  \n\"l_comment\"\n           \nVARCHAR\n(\n44\n),\n\n  \n\"l_dummy\"\n             \nVARCHAR\n(\n10\n));\n\n\n\n\n\nImport Data\n\u00b6\n\n\nSuppose your work directory is \n/home/username/workspace\n and the tpch1g data is stored in \n/home/username/workspace/tpch1g\n. Then, issue the following commands in the PostgreSQL prompt to load the data.\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"region\"\n     from \n'/home/username/workspace/tpch1g/region/region.tbl'\n        DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"nation\"\n     from \n'/home/username/workspace/tpch1g/nation/nation.tbl'\n        DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"customer\"\n   from \n'/home/username/workspace/tpch1g/customer/customer.tbl'\n    DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"supplier\"\n   from \n'/home/username/workspace/tpch1g/supplier/supplier.tbl'\n    DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"part\"\n       from \n'/home/username/workspace/tpch1g/part/part.tbl'\n            DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"partsupp\"\n   from \n'/home/username/workspace/tpch1g/partsupp/partsupp.tbl'\n    DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"orders\"\n     from \n'/home/username/workspace/tpch1g/orders/orders.tbl'\n        DELIMITER \n'|'\n CSV\n;\n\n\n\\c\nopy \n\"tpch1g\"\n.\n\"lineitem\"\n   from \n'/home/username/workspace/tpch1g/lineitem/lineitem.tbl'\n    DELIMITER \n'|'\n CSV\n;\n\n\n\n\n\nApache Spark\n\u00b6\n\n\nPut data to HDFS\n\u00b6\n\n\nUse following commands to put data into HDFS. Suppose the tpch1g data is stored in \n/home/username/workspace/tpch1g\n and you hope to put your data in \n/tmp/tpch1g\n in HDFS.\n\n$ hdfs dfs -mkdir -p /tmp/tpch1g/region\n$ hdfs dfs -mkdir -p /tmp/tpch1g/nation\n$ hdfs dfs -mkdir -p /tmp/tpch1g/customer\n$ hdfs dfs -mkdir -p /tmp/tpch1g/supplier\n$ hdfs dfs -mkdir -p /tmp/tpch1g/part\n$ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp\n$ hdfs dfs -mkdir -p /tmp/tpch1g/orders\n$ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem\n$ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl       /tmp/tpch1g/region\n$ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl       /tmp/tpch1g/nation\n$ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl   /tmp/tpch1g/customer\n$ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl   /tmp/tpch1g/supplier\n$ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl           /tmp/tpch1g/part\n$ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl   /tmp/tpch1g/partsupp\n$ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl       /tmp/tpch1g/orders\n$ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl   /tmp/tpch1g/lineitem\n\n\n\nIf you encounter write permission problem in the next step when creating tables, you can use command\n\n$ hdfs dfs -chmod -R \n777\n /tmp/tpch1g\n\n\nto give full access to your directory.\n\n\nCreate table and load data\n\u00b6\n\n\nSimply copy and paste following queries to spark to set up TPC-H tables.\n\n\n-- nation\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \nnation\n \n(\n\n  \n`\nn_nationkey\n`\n  \nINT\n,\n\n  \n`\nn_name\n`\n       \nCHAR\n(\n25\n),\n\n  \n`\nn_regionkey\n`\n  \nINT\n,\n\n  \n`\nn_comment\n`\n    \nVARCHAR\n(\n152\n),\n\n  \n`\nn_dummy\n`\n      \nVARCHAR\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/nation/nation'\n;\n\n\n\n-- region\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \nregion\n \n(\n\n  \n`\nr_regionkey\n`\n  \nINT\n,\n\n  \n`\nr_name\n`\n       \nCHAR\n(\n25\n),\n\n  \n`\nr_comment\n`\n    \nVARCHAR\n(\n152\n),\n\n  \n`\nr_dummy\n`\n      \nVARCHAR\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/region/region'\n;\n\n\n\n\n-- supplier\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \nsupplier\n \n(\n\n  \n`\ns_suppkey\n`\n     \nINT\n,\n\n  \n`\ns_name\n`\n        \nCHAR\n(\n25\n),\n\n  \n`\ns_address\n`\n     \nVARCHAR\n(\n40\n),\n\n  \n`\ns_nationkey\n`\n   \nINT\n,\n\n  \n`\ns_phone\n`\n       \nCHAR\n(\n15\n),\n\n  \n`\ns_acctbal\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\ns_comment\n`\n     \nVARCHAR\n(\n101\n),\n\n  \n`\ns_dummy\n`\n \nvarchar\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/supplier/supplier'\n;\n\n\n\n\n-- customer\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \ncustomer\n \n(\n\n  \n`\nc_custkey\n`\n     \nINT\n,\n\n  \n`\nc_name\n`\n        \nVARCHAR\n(\n25\n),\n\n  \n`\nc_address\n`\n     \nVARCHAR\n(\n40\n),\n\n  \n`\nc_nationkey\n`\n   \nINT\n,\n\n  \n`\nc_phone\n`\n       \nCHAR\n(\n15\n),\n\n  \n`\nc_acctbal\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nc_mktsegment\n`\n  \nCHAR\n(\n10\n),\n\n  \n`\nc_comment\n`\n     \nVARCHAR\n(\n117\n),\n\n  \n`\nc_dummy\n`\n       \nVARCHAR\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/customer/customer'\n;\n\n\n\n\n-- part\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \npart\n \n(\n\n  \n`\np_partkey\n`\n     \nINT\n,\n\n  \n`\np_name\n`\n        \nVARCHAR\n(\n55\n),\n\n  \n`\np_mfgr\n`\n        \nCHAR\n(\n25\n),\n\n  \n`\np_brand\n`\n       \nCHAR\n(\n10\n),\n\n  \n`\np_type\n`\n        \nVARCHAR\n(\n25\n),\n\n  \n`\np_size\n`\n        \nINT\n,\n\n  \n`\np_container\n`\n   \nCHAR\n(\n10\n),\n\n  \n`\np_retailprice\n`\n \nDECIMAL\n(\n15\n,\n2\n)\n \n,\n\n  \n`\np_comment\n`\n     \nVARCHAR\n(\n23\n)\n \n,\n\n  \n`\np_dummy\n`\n       \nVARCHAR\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/part/part'\n;\n\n\n\n-- partsupp\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \npartsupp\n \n(\n\n  \n`\nps_partkey\n`\n     \nINT\n,\n\n  \n`\nps_suppkey\n`\n     \nINT\n,\n\n  \n`\nps_availqty\n`\n    \nINT\n,\n\n  \n`\nps_supplycost\n`\n  \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nps_comment\n`\n     \nVARCHAR\n(\n199\n),\n\n  \n`\nps_dummy\n`\n       \nVARCHAR\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/partsupp/partsupp'\n;\n\n\n\n-- orders\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \norders\n \n(\n\n  \n`\no_orderkey\n`\n       \nINT\n,\n\n  \n`\no_custkey\n`\n        \nINT\n,\n\n  \n`\no_orderstatus\n`\n    \nCHAR\n(\n1\n),\n\n  \n`\no_totalprice\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\no_orderdate\n`\n      \nDATE\n,\n\n  \n`\no_orderpriority\n`\n  \nCHAR\n(\n15\n),\n\n  \n`\no_clerk\n`\n          \nCHAR\n(\n15\n),\n\n  \n`\no_shippriority\n`\n   \nINT\n,\n\n  \n`\no_comment\n`\n        \nVARCHAR\n(\n79\n),\n\n  \n`\no_dummy\n`\n          \nVARCHAR\n(\n10\n))\n\n   \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n   \nSTORED\n \nAS\n \nTEXTFILE\n\n   \nLOCATION\n \n'/tmp/tpch1g/orders/orders'\n;\n\n\n\n-- lineitem\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \nlineitem\n \n(\n\n  \n`\nl_orderkey\n`\n          \nINT\n,\n\n  \n`\nl_partkey\n`\n           \nINT\n,\n\n  \n`\nl_suppkey\n`\n           \nINT\n,\n\n  \n`\nl_linenumber\n`\n        \nINT\n,\n\n  \n`\nl_quantity\n`\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_extendedprice\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_discount\n`\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_tax\n`\n               \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_returnflag\n`\n        \nCHAR\n(\n1\n),\n\n  \n`\nl_linestatus\n`\n        \nCHAR\n(\n1\n),\n\n  \n`\nl_shipdate\n`\n          \nDATE\n,\n\n  \n`\nl_commitdate\n`\n        \nDATE\n,\n\n  \n`\nl_receiptdate\n`\n       \nDATE\n,\n\n  \n`\nl_shipinstruct\n`\n      \nCHAR\n(\n25\n),\n\n  \n`\nl_shipmode\n`\n          \nCHAR\n(\n10\n),\n\n  \n`\nl_comment\n`\n           \nVARCHAR\n(\n44\n),\n\n  \n`\nl_dummy\n`\n             \nVARCHAR\n(\n10\n))\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nSTORED\n \nAS\n \nTEXTFILE\n\n  \nLOCATION\n \n'/tmp/tpch1g/lineitem/lineitem'\n;\n\n\n\n\n\nRedshift\n\u00b6\n\n\nCreate tables\n\u00b6\n\n\nUse SQL query tools like \nSQL Workbench/J\n to connect with your Redshift Cluster.\n\n\nFirst, Create a schema for testing\n\ncreate\n \nschema\n \n\"tpch1g\"\n;\n\n\n\nThen Create empty tables.\n\n-- nation\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"nation\"\n \n(\n\n  \n\"n_nationkey\"\n  \nINT\n,\n\n  \n\"n_name\"\n       \nCHAR\n(\n25\n),\n\n  \n\"n_regionkey\"\n  \nINT\n,\n\n  \n\"n_comment\"\n    \nVARCHAR\n(\n152\n),\n\n  \n\"n_dummy\"\n      \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"n_nationkey\"\n));\n\n\n\n-- region\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"region\"\n \n(\n\n  \n\"r_regionkey\"\n  \nINT\n,\n\n  \n\"r_name\"\n       \nCHAR\n(\n25\n),\n\n  \n\"r_comment\"\n    \nVARCHAR\n(\n152\n),\n\n  \n\"r_dummy\"\n      \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"r_regionkey\"\n));\n\n\n\n-- supplier\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"supplier\"\n \n(\n\n  \n\"s_suppkey\"\n     \nINT\n,\n\n  \n\"s_name\"\n        \nCHAR\n(\n25\n),\n\n  \n\"s_address\"\n     \nVARCHAR\n(\n40\n),\n\n  \n\"s_nationkey\"\n   \nINT\n,\n\n  \n\"s_phone\"\n       \nCHAR\n(\n15\n),\n\n  \n\"s_acctbal\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"s_comment\"\n     \nVARCHAR\n(\n101\n),\n\n  \n\"s_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"s_suppkey\"\n));\n\n\n\n-- customer\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"customer\"\n \n(\n\n  \n\"c_custkey\"\n     \nINT\n,\n\n  \n\"c_name\"\n        \nVARCHAR\n(\n25\n),\n\n  \n\"c_address\"\n     \nVARCHAR\n(\n40\n),\n\n  \n\"c_nationkey\"\n   \nINT\n,\n\n  \n\"c_phone\"\n       \nCHAR\n(\n15\n),\n\n  \n\"c_acctbal\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"c_mktsegment\"\n  \nCHAR\n(\n10\n),\n\n  \n\"c_comment\"\n     \nVARCHAR\n(\n117\n),\n\n  \n\"c_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"c_custkey\"\n));\n\n\n\n-- part\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"part\"\n \n(\n\n  \n\"p_partkey\"\n     \nINT\n,\n\n  \n\"p_name\"\n        \nVARCHAR\n(\n55\n),\n\n  \n\"p_mfgr\"\n        \nCHAR\n(\n25\n),\n\n  \n\"p_brand\"\n       \nCHAR\n(\n10\n),\n\n  \n\"p_type\"\n        \nVARCHAR\n(\n25\n),\n\n  \n\"p_size\"\n        \nINT\n,\n\n  \n\"p_container\"\n   \nCHAR\n(\n10\n),\n\n  \n\"p_retailprice\"\n \nDECIMAL\n(\n15\n,\n2\n)\n \n,\n\n  \n\"p_comment\"\n     \nVARCHAR\n(\n23\n)\n \n,\n\n  \n\"p_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"p_partkey\"\n));\n\n\n\n-- partsupp\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"partsupp\"\n \n(\n\n  \n\"ps_partkey\"\n     \nINT\n,\n\n  \n\"ps_suppkey\"\n     \nINT\n,\n\n  \n\"ps_availqty\"\n    \nINT\n,\n\n  \n\"ps_supplycost\"\n  \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"ps_comment\"\n     \nVARCHAR\n(\n199\n),\n\n  \n\"ps_dummy\"\n       \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"ps_partkey\"\n));\n\n\n\n-- orders\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"orders\"\n \n(\n\n  \n\"o_orderkey\"\n       \nINT\n,\n\n  \n\"o_custkey\"\n        \nINT\n,\n\n  \n\"o_orderstatus\"\n    \nCHAR\n(\n1\n),\n\n  \n\"o_totalprice\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"o_orderdate\"\n      \nDATE\n,\n\n  \n\"o_orderpriority\"\n  \nCHAR\n(\n15\n),\n\n  \n\"o_clerk\"\n          \nCHAR\n(\n15\n),\n\n  \n\"o_shippriority\"\n   \nINT\n,\n\n  \n\"o_comment\"\n        \nVARCHAR\n(\n79\n),\n\n  \n\"o_dummy\"\n          \nVARCHAR\n(\n10\n),\n\n  \nPRIMARY\n \nKEY\n \n(\n\"o_orderkey\"\n));\n\n\n\n-- lineitem\n\n\nCREATE\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n\"tpch1g\"\n.\n\"lineitem\"\n(\n\n  \n\"l_orderkey\"\n          \nINT\n,\n\n  \n\"l_partkey\"\n           \nINT\n,\n\n  \n\"l_suppkey\"\n           \nINT\n,\n\n  \n\"l_linenumber\"\n        \nINT\n,\n\n  \n\"l_quantity\"\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_extendedprice\"\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_discount\"\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_tax\"\n               \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n\"l_returnflag\"\n        \nCHAR\n(\n1\n),\n\n  \n\"l_linestatus\"\n        \nCHAR\n(\n1\n),\n\n  \n\"l_shipdate\"\n          \nDATE\n,\n\n  \n\"l_commitdate\"\n        \nDATE\n,\n\n  \n\"l_receiptdate\"\n       \nDATE\n,\n\n  \n\"l_shipinstruct\"\n      \nCHAR\n(\n25\n),\n\n  \n\"l_shipmode\"\n          \nCHAR\n(\n10\n),\n\n  \n\"l_comment\"\n           \nVARCHAR\n(\n44\n),\n\n  \n\"l_dummy\"\n             \nVARCHAR\n(\n10\n));\n\n\n\n\nLoad Data\n\u00b6\n\n\nFor Redshift, we use Java method to help inserting our data into Redshift tables. \nschema\n is the Redshift schema you create your TPC-H tables\nand \ntable\n is the table name of TPCH-table, such as \nnation\n, \nregion\n. \nconn\n is the Connection class you get from connecting your Redshift cluster using Redshift JDBC driver.\nSuppose your work directory is \n/home/username/workspace\n and the tpch1g data is stored in \n/home/username/workspace/tpch1g\n.\n\nstatic\n \nvoid\n \nloadRedshiftData\n(\nString\n \nschema\n,\n \nString\n \ntable\n,\n \nConnection\n \nconn\n)\n\n      \nthrows\n \nIOException\n \n{\n\n    \nStatement\n \nstmt\n \n=\n \nconn\n.\ncreateStatement\n();\n\n    \nString\n \nconcat\n \n=\n \n\"\"\n;\n\n    \nFile\n \nfile\n \n=\n\n        \nnew\n \nFile\n(\nString\n.\nformat\n(\n\"/home/username/workspace/tpch1g/%s/%s.tbl\"\n,\n \ntable\n,\n \ntable\n));\n\n    \nResultSet\n \ncolumnMeta\n \n=\n\n        \nstmt\n.\nexecute\n(\n\n            \nString\n.\nformat\n(\n\n                \n\"select data_type, ordinal_position from INFORMATION_SCHEMA.COLUMNS where table_name='%s' and table_schema='%s'\"\n,\n\n                \ntable\n,\n \nschema\n));\n\n    \nList\n<\nBoolean\n>\n \nquotedNeeded\n \n=\n \nnew\n \nArrayList\n<>();\n\n    \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n<\n \ncolumnMeta\n.\ngetRowCount\n();\n \ni\n++)\n \n{\n\n      \nquotedNeeded\n.\nadd\n(\ntrue\n);\n\n    \n}\n\n    \nwhile\n \n(\ncolumnMeta\n.\nnext\n())\n \n{\n\n      \nString\n \ncolumnType\n \n=\n \ncolumnMeta\n.\ngetString\n(\n1\n);\n\n      \nint\n \ncolumnIndex\n \n=\n \ncolumnMeta\n.\ngetInt\n(\n2\n);\n\n      \nif\n \n(\ncolumnType\n.\nequals\n(\n\"integer\"\n)\n \n||\n \ncolumnType\n.\nequals\n(\n\"numeric\"\n))\n \n{\n\n        \nquotedNeeded\n.\nset\n(\ncolumnIndex\n \n-\n \n1\n,\n \nfalse\n);\n\n      \n}\n\n    \n}\n\n\n    \nString\n \ncontent\n \n=\n \nFiles\n.\ntoString\n(\nfile\n,\n \nCharsets\n.\nUTF_8\n);\n\n    \nfor\n \n(\nString\n \nrow\n \n:\n \ncontent\n.\nsplit\n(\n\"\\n\"\n))\n \n{\n\n      \nString\n[]\n \nvalues\n \n=\n \nrow\n.\nsplit\n(\n\"\\\\|\"\n);\n\n      \nrow\n \n=\n \n\"\"\n;\n\n      \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n<\n \nvalues\n.\nlength\n \n-\n \n1\n;\n \ni\n++)\n \n{\n\n        \nif\n \n(\nquotedNeeded\n.\nget\n(\ni\n))\n \n{\n\n          \nrow\n \n=\n \nrow\n \n+\n \ngetQuoted\n(\nvalues\n[\ni\n])\n \n+\n \n\",\"\n;\n\n        \n}\n \nelse\n \n{\n\n          \nrow\n \n=\n \nrow\n \n+\n \nvalues\n[\ni\n]\n \n+\n \n\",\"\n;\n\n        \n}\n\n      \n}\n\n      \nrow\n \n=\n \nrow\n \n+\n \n\"''\"\n;\n\n      \nif\n \n(\nconcat\n.\nequals\n(\n\"\"\n))\n \n{\n\n        \nconcat\n \n=\n \nconcat\n \n+\n \n\"(\"\n \n+\n \nrow\n \n+\n \n\")\"\n;\n\n      \n}\n \nelse\n \nconcat\n \n=\n \nconcat\n \n+\n \n\",\"\n \n+\n \n\"(\"\n \n+\n \nrow\n \n+\n \n\")\"\n;\n\n    \n}\n\n    \nstmt\n.\nexecute\n(\nString\n.\nformat\n(\n\"insert into \\\"%s\\\".\\\"%s\\\" values %s\"\n,\n \nschema\n,\n \ntable\n,\n \nconcat\n));\n\n  \n}\n\n\n\n\nCloudera Impala\n\u00b6\n\n\nPut data to HDFS\n\u00b6\n\n\nUse following commands to put data into HDFS. Suppose the tpch1g data is stored in \n/home/username/workspace/tpch1g\n and you hope to put your data in \n/tmp/tpch1g\n in HDFS.\n\n$ hdfs dfs -mkdir -p /tmp/tpch1g/region\n$ hdfs dfs -mkdir -p /tmp/tpch1g/nation\n$ hdfs dfs -mkdir -p /tmp/tpch1g/customer\n$ hdfs dfs -mkdir -p /tmp/tpch1g/supplier\n$ hdfs dfs -mkdir -p /tmp/tpch1g/part\n$ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp\n$ hdfs dfs -mkdir -p /tmp/tpch1g/orders\n$ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem\n$ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl       /tmp/tpch1g/region\n$ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl       /tmp/tpch1g/nation\n$ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl   /tmp/tpch1g/customer\n$ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl   /tmp/tpch1g/supplier\n$ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl           /tmp/tpch1g/part\n$ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl   /tmp/tpch1g/partsupp\n$ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl       /tmp/tpch1g/orders\n$ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl   /tmp/tpch1g/lineitem\n\n\n\nIf you encounter write permission problem in the next step when creating tables, you can use command\n\n$ hdfs dfs -chmod -R \n777\n /tmp/tpch1g\n\n\nto give full access to your directory.\n\n\nCreate tables and load data\n\u00b6\n\n\nConnect to Impala.\n\n\n$ impala-shell\n\n\n\n\nCreate a schema for testing.\n\n\ncreate schema `tpch1g`;\n\n\n\n\nCreate tables and load data. Simply copy and paste the following table definition statements into the Impala shell.\n\n\n-- nation\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\nnation\n`\n \n(\n\n  \n`\nn_nationkey\n`\n  \nINT\n,\n\n  \n`\nn_name\n`\n       \nSTRING\n,\n\n  \n`\nn_regionkey\n`\n  \nINT\n,\n\n  \n`\nn_comment\n`\n    \nSTRING\n,\n\n  \n`\nn_dummy\n`\n      \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/nation'\n;\n\n\n\n-- region\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\nregion\n`\n \n(\n\n  \n`\nr_regionkey\n`\n  \nINT\n,\n\n  \n`\nr_name\n`\n       \nSTRING\n,\n\n  \n`\nr_comment\n`\n    \nSTRING\n,\n\n  \n`\nr_dummy\n`\n      \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/region'\n;\n\n\n\n-- supplier\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\nsupplier\n`\n \n(\n\n  \n`\ns_suppkey\n`\n     \nINT\n,\n\n  \n`\ns_name\n`\n        \nSTRING\n,\n\n  \n`\ns_address\n`\n     \nSTRING\n,\n\n  \n`\ns_nationkey\n`\n   \nINT\n,\n\n  \n`\ns_phone\n`\n       \nSTRING\n,\n\n  \n`\ns_acctbal\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\ns_comment\n`\n     \nSTRING\n,\n\n  \n`\ns_dummy\n`\n       \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/supplier'\n;\n\n\n\n-- customer\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\ncustomer\n`\n \n(\n\n  \n`\nc_custkey\n`\n     \nINT\n,\n\n  \n`\nc_name\n`\n        \nSTRING\n,\n\n  \n`\nc_address\n`\n     \nSTRING\n,\n\n  \n`\nc_nationkey\n`\n   \nINT\n,\n\n  \n`\nc_phone\n`\n       \nSTRING\n,\n\n  \n`\nc_acctbal\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nc_mktsegment\n`\n  \nSTRING\n,\n\n  \n`\nc_comment\n`\n     \nSTRING\n,\n\n  \n`\nc_dummy\n`\n       \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/customer'\n;\n\n\n\n-- part\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\npart\n`\n \n(\n\n  \n`\np_partkey\n`\n     \nINT\n,\n\n  \n`\np_name\n`\n        \nSTRING\n,\n\n  \n`\np_mfgr\n`\n        \nSTRING\n,\n\n  \n`\np_brand\n`\n       \nSTRING\n,\n\n  \n`\np_type\n`\n        \nSTRING\n,\n\n  \n`\np_size\n`\n        \nINT\n,\n\n  \n`\np_container\n`\n   \nSTRING\n,\n\n  \n`\np_retailprice\n`\n \nDECIMAL\n(\n15\n,\n2\n)\n \n,\n\n  \n`\np_comment\n`\n     \nSTRING\n \n,\n\n  \n`\np_dummy\n`\n       \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/part'\n;\n\n\n\n-- partsupp\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\npartsupp\n`\n \n(\n\n  \n`\nps_partkey\n`\n     \nINT\n,\n\n  \n`\nps_suppkey\n`\n     \nINT\n,\n\n  \n`\nps_availqty\n`\n    \nINT\n,\n\n  \n`\nps_supplycost\n`\n  \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nps_comment\n`\n     \nSTRING\n,\n\n  \n`\nps_dummy\n`\n       \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/partsupp'\n;\n\n\n\n-- orders\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\norders\n`\n \n(\n\n  \n`\no_orderkey\n`\n       \nINT\n,\n\n  \n`\no_custkey\n`\n        \nINT\n,\n\n  \n`\no_orderstatus\n`\n    \nSTRING\n,\n\n  \n`\no_totalprice\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\no_orderdate\n`\n      \nTIMESTAMP\n,\n\n  \n`\no_orderpriority\n`\n  \nSTRING\n,\n\n  \n`\no_clerk\n`\n          \nSTRING\n,\n\n  \n`\no_shippriority\n`\n   \nINT\n,\n\n  \n`\no_comment\n`\n        \nSTRING\n,\n\n  \n`\no_dummy\n`\n          \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/orders'\n;\n\n\n\n-- lineitem\n\n\nCREATE\n \nEXTERNAL\n \nTABLE\n \nIF\n \nNOT\n \nEXISTS\n \n`\ntpch1g\n`\n.\n`\nlineitem\n`\n(\n\n  \n`\nl_orderkey\n`\n          \nINT\n,\n\n  \n`\nl_partkey\n`\n           \nINT\n,\n\n  \n`\nl_suppkey\n`\n           \nINT\n,\n\n  \n`\nl_linenumber\n`\n        \nINT\n,\n\n  \n`\nl_quantity\n`\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_extendedprice\n`\n     \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_discount\n`\n          \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_tax\n`\n               \nDECIMAL\n(\n15\n,\n2\n),\n\n  \n`\nl_returnflag\n`\n        \nSTRING\n,\n\n  \n`\nl_linestatus\n`\n        \nSTRING\n,\n\n  \n`\nl_shipdate\n`\n          \nTIMESTAMP\n,\n\n  \n`\nl_commitdate\n`\n        \nTIMESTAMP\n,\n\n  \n`\nl_receiptdate\n`\n       \nTIMESTAMP\n,\n\n  \n`\nl_shipinstruct\n`\n      \nSTRING\n,\n\n  \n`\nl_shipmode\n`\n          \nSTRING\n,\n\n  \n`\nl_comment\n`\n           \nSTRING\n,\n\n  \n`\nl_dummy\n`\n             \nSTRING\n)\n\n  \nROW\n \nFORMAT\n \nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n'|'\n\n  \nLOCATION\n \n'/tmp/tpch1g/lineitem'\n;",
            "title": "Setup TPC-H Data"
        },
        {
            "location": "/tutorial/tpch/#tpc-h-data-setup",
            "text": "This is a step-by-step guide for setting up TPC-H data in different databases. This guide will use 1GB data. This guide assumes you have basic knowledge about issuing commands in a terminal application.",
            "title": "TPC-H Data Setup"
        },
        {
            "location": "/tutorial/tpch/#download-data",
            "text": "Go to your work directory (say  /home/username/workspace ) and download the data:  cd  /home/username/workspace\ncurl http://dbgroup-internal.eecs.umich.edu/projects/verdictdb/tpch1g.zip -o tpch1g.zip  Unzip the downloaded file:  unzip tpch1g.zip  It will create a new directory named  tpch1g  under your work directory. The directory contains 8 sub-directories for each of 8 tables.",
            "title": "Download Data"
        },
        {
            "location": "/tutorial/tpch/#mysql",
            "text": "",
            "title": "MySQL"
        },
        {
            "location": "/tutorial/tpch/#create-tables",
            "text": "Connect to your MySQL database.  mysql -uroot  Create a schema for test.  mysql> create database tpch1g ;   Create empty tables; simply copy and paste the following table definition statements into the MySQL shell. We will import the data later into these tables.  -- nation  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . nation   ( \n   ` n_nationkey `    INT , \n   ` n_name `         CHAR ( 25 ), \n   ` n_regionkey `    INT , \n   ` n_comment `      VARCHAR ( 152 ), \n   ` n_dummy `        VARCHAR ( 10 ), \n   PRIMARY   KEY   ( ` n_nationkey ` ));  -- region  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . region   ( \n   ` r_regionkey `    INT , \n   ` r_name `         CHAR ( 25 ), \n   ` r_comment `      VARCHAR ( 152 ), \n   ` r_dummy `        VARCHAR ( 10 ), \n   PRIMARY   KEY   ( ` r_regionkey ` ));  -- supplier  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . supplier   ( \n   ` s_suppkey `       INT , \n   ` s_name `          CHAR ( 25 ), \n   ` s_address `       VARCHAR ( 40 ), \n   ` s_nationkey `     INT , \n   ` s_phone `         CHAR ( 15 ), \n   ` s_acctbal `       DECIMAL ( 15 , 2 ), \n   ` s_comment `       VARCHAR ( 101 ), \n   ` s_dummy `   varchar ( 10 ), \n   PRIMARY   KEY   ( ` s_suppkey ` ));  -- customer  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . customer   ( \n   ` c_custkey `       INT , \n   ` c_name `          VARCHAR ( 25 ), \n   ` c_address `       VARCHAR ( 40 ), \n   ` c_nationkey `     INT , \n   ` c_phone `         CHAR ( 15 ), \n   ` c_acctbal `       DECIMAL ( 15 , 2 ), \n   ` c_mktsegment `    CHAR ( 10 ), \n   ` c_comment `       VARCHAR ( 117 ), \n   ` c_dummy `         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( ` c_custkey ` ));  -- part  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . part   ( \n   ` p_partkey `       INT , \n   ` p_name `          VARCHAR ( 55 ), \n   ` p_mfgr `          CHAR ( 25 ), \n   ` p_brand `         CHAR ( 10 ), \n   ` p_type `          VARCHAR ( 25 ), \n   ` p_size `          INT , \n   ` p_container `     CHAR ( 10 ), \n   ` p_retailprice `   DECIMAL ( 15 , 2 )   , \n   ` p_comment `       VARCHAR ( 23 )   , \n   ` p_dummy `         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( ` p_partkey ` ));  -- partsupp  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . partsupp   ( \n   ` ps_partkey `       INT , \n   ` ps_suppkey `       INT , \n   ` ps_availqty `      INT , \n   ` ps_supplycost `    DECIMAL ( 15 , 2 ), \n   ` ps_comment `       VARCHAR ( 199 ), \n   ` ps_dummy `         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( ` ps_partkey ` ));  -- orders  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . orders   ( \n   ` o_orderkey `         INT , \n   ` o_custkey `          INT , \n   ` o_orderstatus `      CHAR ( 1 ), \n   ` o_totalprice `       DECIMAL ( 15 , 2 ), \n   ` o_orderdate `        DATE , \n   ` o_orderpriority `    CHAR ( 15 ), \n   ` o_clerk `            CHAR ( 15 ), \n   ` o_shippriority `     INT , \n   ` o_comment `          VARCHAR ( 79 ), \n   ` o_dummy `            VARCHAR ( 10 ), \n   PRIMARY   KEY   ( ` o_orderkey ` ));  -- lineitem  CREATE   TABLE   IF   NOT   EXISTS   tpch1g . lineitem   ( \n   ` l_orderkey `      INT , \n   ` l_partkey `       INT , \n   ` l_suppkey `       INT , \n   ` l_linenumber `    INT , \n   ` l_quantity `      DECIMAL ( 15 , 2 ), \n   ` l_extendedprice `    DECIMAL ( 15 , 2 ), \n   ` l_discount `      DECIMAL ( 15 , 2 ), \n   ` l_tax `           DECIMAL ( 15 , 2 ), \n   ` l_returnflag `    CHAR ( 1 ), \n   ` l_linestatus `    CHAR ( 1 ), \n   ` l_shipdate `      DATE , \n   ` l_commitdate `    DATE , \n   ` l_receiptdate `   DATE , \n   ` l_shipinstruct `   CHAR ( 25 ), \n   ` l_shipmode `      CHAR ( 10 ), \n   ` l_comment `       VARCHAR ( 44 ), \n   ` l_dummy `         VARCHAR ( 10 ));",
            "title": "Create tables"
        },
        {
            "location": "/tutorial/tpch/#import-data",
            "text": "Suppose your work directory is  /home/username/workspace  and the tpch1g data is stored in  /home/username/workspace/tpch1g . Then, issue the following commands in the MySQL shell to load the data.  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/region/region.tbl'       INTO   TABLE   region       FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/nation/nation.tbl'       INTO   TABLE   nation       FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/customer/customer.tbl'   INTO   TABLE   customer     FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/supplier/supplier.tbl'   INTO   TABLE   supplier     FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/part/part.tbl'           INTO   TABLE   part         FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/partsupp/partsupp.tbl'   INTO   TABLE   partsupp     FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/orders/orders.tbl'       INTO   TABLE   orders       FIELDS   TERMINATED   BY   '|' ;  LOAD   DATA   LOCAL   INFILE   '/home/username/workspace/tpch1g/lineitem/lineitem.tbl'   INTO   TABLE   lineitem     FIELDS   TERMINATED   BY   '|' ;",
            "title": "Import Data"
        },
        {
            "location": "/tutorial/tpch/#postgresql",
            "text": "",
            "title": "PostgreSQL"
        },
        {
            "location": "/tutorial/tpch/#create-tables_1",
            "text": "Connect to your Postgresql database.  psql  Create a schema for testing.  postgres=# create schema \"tpch1g\";  Create empty tables; simply copy and paste the following table definition statements into the PostgreSQL prompt. We will import the data later into these tables.  -- nation  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"nation\"   ( \n   \"n_nationkey\"    INT , \n   \"n_name\"         CHAR ( 25 ), \n   \"n_regionkey\"    INT , \n   \"n_comment\"      VARCHAR ( 152 ), \n   \"n_dummy\"        VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"n_nationkey\" ));  -- region  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"region\"   ( \n   \"r_regionkey\"    INT , \n   \"r_name\"         CHAR ( 25 ), \n   \"r_comment\"      VARCHAR ( 152 ), \n   \"r_dummy\"        VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"r_regionkey\" ));  -- supplier  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"supplier\"   ( \n   \"s_suppkey\"       INT , \n   \"s_name\"          CHAR ( 25 ), \n   \"s_address\"       VARCHAR ( 40 ), \n   \"s_nationkey\"     INT , \n   \"s_phone\"         CHAR ( 15 ), \n   \"s_acctbal\"       DECIMAL ( 15 , 2 ), \n   \"s_comment\"       VARCHAR ( 101 ), \n   \"s_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"s_suppkey\" ));  -- customer  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"customer\"   ( \n   \"c_custkey\"       INT , \n   \"c_name\"          VARCHAR ( 25 ), \n   \"c_address\"       VARCHAR ( 40 ), \n   \"c_nationkey\"     INT , \n   \"c_phone\"         CHAR ( 15 ), \n   \"c_acctbal\"       DECIMAL ( 15 , 2 ), \n   \"c_mktsegment\"    CHAR ( 10 ), \n   \"c_comment\"       VARCHAR ( 117 ), \n   \"c_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"c_custkey\" ));  -- part  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"part\"   ( \n   \"p_partkey\"       INT , \n   \"p_name\"          VARCHAR ( 55 ), \n   \"p_mfgr\"          CHAR ( 25 ), \n   \"p_brand\"         CHAR ( 10 ), \n   \"p_type\"          VARCHAR ( 25 ), \n   \"p_size\"          INT , \n   \"p_container\"     CHAR ( 10 ), \n   \"p_retailprice\"   DECIMAL ( 15 , 2 )   , \n   \"p_comment\"       VARCHAR ( 23 )   , \n   \"p_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"p_partkey\" ));  -- partsupp  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"partsupp\"   ( \n   \"ps_partkey\"       INT , \n   \"ps_suppkey\"       INT , \n   \"ps_availqty\"      INT , \n   \"ps_supplycost\"    DECIMAL ( 15 , 2 ), \n   \"ps_comment\"       VARCHAR ( 199 ), \n   \"ps_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"ps_partkey\" ));  -- orders  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"orders\"   ( \n   \"o_orderkey\"         INT , \n   \"o_custkey\"          INT , \n   \"o_orderstatus\"      CHAR ( 1 ), \n   \"o_totalprice\"       DECIMAL ( 15 , 2 ), \n   \"o_orderdate\"        DATE , \n   \"o_orderpriority\"    CHAR ( 15 ), \n   \"o_clerk\"            CHAR ( 15 ), \n   \"o_shippriority\"     INT , \n   \"o_comment\"          VARCHAR ( 79 ), \n   \"o_dummy\"            VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"o_orderkey\" ));  -- lineitem  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"lineitem\" ( \n   \"l_orderkey\"            INT , \n   \"l_partkey\"             INT , \n   \"l_suppkey\"             INT , \n   \"l_linenumber\"          INT , \n   \"l_quantity\"            DECIMAL ( 15 , 2 ), \n   \"l_extendedprice\"       DECIMAL ( 15 , 2 ), \n   \"l_discount\"            DECIMAL ( 15 , 2 ), \n   \"l_tax\"                 DECIMAL ( 15 , 2 ), \n   \"l_returnflag\"          CHAR ( 1 ), \n   \"l_linestatus\"          CHAR ( 1 ), \n   \"l_shipdate\"            DATE , \n   \"l_commitdate\"          DATE , \n   \"l_receiptdate\"         DATE , \n   \"l_shipinstruct\"        CHAR ( 25 ), \n   \"l_shipmode\"            CHAR ( 10 ), \n   \"l_comment\"             VARCHAR ( 44 ), \n   \"l_dummy\"               VARCHAR ( 10 ));",
            "title": "Create tables"
        },
        {
            "location": "/tutorial/tpch/#import-data_1",
            "text": "Suppose your work directory is  /home/username/workspace  and the tpch1g data is stored in  /home/username/workspace/tpch1g . Then, issue the following commands in the PostgreSQL prompt to load the data.  \\c opy  \"tpch1g\" . \"region\"      from  '/home/username/workspace/tpch1g/region/region.tbl'         DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"nation\"      from  '/home/username/workspace/tpch1g/nation/nation.tbl'         DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"customer\"    from  '/home/username/workspace/tpch1g/customer/customer.tbl'     DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"supplier\"    from  '/home/username/workspace/tpch1g/supplier/supplier.tbl'     DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"part\"        from  '/home/username/workspace/tpch1g/part/part.tbl'             DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"partsupp\"    from  '/home/username/workspace/tpch1g/partsupp/partsupp.tbl'     DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"orders\"      from  '/home/username/workspace/tpch1g/orders/orders.tbl'         DELIMITER  '|'  CSV ;  \\c opy  \"tpch1g\" . \"lineitem\"    from  '/home/username/workspace/tpch1g/lineitem/lineitem.tbl'     DELIMITER  '|'  CSV ;",
            "title": "Import Data"
        },
        {
            "location": "/tutorial/tpch/#apache-spark",
            "text": "",
            "title": "Apache Spark"
        },
        {
            "location": "/tutorial/tpch/#put-data-to-hdfs",
            "text": "Use following commands to put data into HDFS. Suppose the tpch1g data is stored in  /home/username/workspace/tpch1g  and you hope to put your data in  /tmp/tpch1g  in HDFS. $ hdfs dfs -mkdir -p /tmp/tpch1g/region\n$ hdfs dfs -mkdir -p /tmp/tpch1g/nation\n$ hdfs dfs -mkdir -p /tmp/tpch1g/customer\n$ hdfs dfs -mkdir -p /tmp/tpch1g/supplier\n$ hdfs dfs -mkdir -p /tmp/tpch1g/part\n$ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp\n$ hdfs dfs -mkdir -p /tmp/tpch1g/orders\n$ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem\n$ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl       /tmp/tpch1g/region\n$ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl       /tmp/tpch1g/nation\n$ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl   /tmp/tpch1g/customer\n$ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl   /tmp/tpch1g/supplier\n$ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl           /tmp/tpch1g/part\n$ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl   /tmp/tpch1g/partsupp\n$ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl       /tmp/tpch1g/orders\n$ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl   /tmp/tpch1g/lineitem  If you encounter write permission problem in the next step when creating tables, you can use command $ hdfs dfs -chmod -R  777  /tmp/tpch1g \nto give full access to your directory.",
            "title": "Put data to HDFS"
        },
        {
            "location": "/tutorial/tpch/#create-table-and-load-data",
            "text": "Simply copy and paste following queries to spark to set up TPC-H tables.  -- nation  CREATE   TABLE   IF   NOT   EXISTS   nation   ( \n   ` n_nationkey `    INT , \n   ` n_name `         CHAR ( 25 ), \n   ` n_regionkey `    INT , \n   ` n_comment `      VARCHAR ( 152 ), \n   ` n_dummy `        VARCHAR ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/nation/nation' ;  -- region  CREATE   TABLE   IF   NOT   EXISTS   region   ( \n   ` r_regionkey `    INT , \n   ` r_name `         CHAR ( 25 ), \n   ` r_comment `      VARCHAR ( 152 ), \n   ` r_dummy `        VARCHAR ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/region/region' ;  -- supplier  CREATE   TABLE   IF   NOT   EXISTS   supplier   ( \n   ` s_suppkey `       INT , \n   ` s_name `          CHAR ( 25 ), \n   ` s_address `       VARCHAR ( 40 ), \n   ` s_nationkey `     INT , \n   ` s_phone `         CHAR ( 15 ), \n   ` s_acctbal `       DECIMAL ( 15 , 2 ), \n   ` s_comment `       VARCHAR ( 101 ), \n   ` s_dummy `   varchar ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/supplier/supplier' ;  -- customer  CREATE   TABLE   IF   NOT   EXISTS   customer   ( \n   ` c_custkey `       INT , \n   ` c_name `          VARCHAR ( 25 ), \n   ` c_address `       VARCHAR ( 40 ), \n   ` c_nationkey `     INT , \n   ` c_phone `         CHAR ( 15 ), \n   ` c_acctbal `       DECIMAL ( 15 , 2 ), \n   ` c_mktsegment `    CHAR ( 10 ), \n   ` c_comment `       VARCHAR ( 117 ), \n   ` c_dummy `         VARCHAR ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/customer/customer' ;  -- part  CREATE   TABLE   IF   NOT   EXISTS   part   ( \n   ` p_partkey `       INT , \n   ` p_name `          VARCHAR ( 55 ), \n   ` p_mfgr `          CHAR ( 25 ), \n   ` p_brand `         CHAR ( 10 ), \n   ` p_type `          VARCHAR ( 25 ), \n   ` p_size `          INT , \n   ` p_container `     CHAR ( 10 ), \n   ` p_retailprice `   DECIMAL ( 15 , 2 )   , \n   ` p_comment `       VARCHAR ( 23 )   , \n   ` p_dummy `         VARCHAR ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/part/part' ;  -- partsupp  CREATE   TABLE   IF   NOT   EXISTS   partsupp   ( \n   ` ps_partkey `       INT , \n   ` ps_suppkey `       INT , \n   ` ps_availqty `      INT , \n   ` ps_supplycost `    DECIMAL ( 15 , 2 ), \n   ` ps_comment `       VARCHAR ( 199 ), \n   ` ps_dummy `         VARCHAR ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/partsupp/partsupp' ;  -- orders  CREATE   TABLE   IF   NOT   EXISTS   orders   ( \n   ` o_orderkey `         INT , \n   ` o_custkey `          INT , \n   ` o_orderstatus `      CHAR ( 1 ), \n   ` o_totalprice `       DECIMAL ( 15 , 2 ), \n   ` o_orderdate `        DATE , \n   ` o_orderpriority `    CHAR ( 15 ), \n   ` o_clerk `            CHAR ( 15 ), \n   ` o_shippriority `     INT , \n   ` o_comment `          VARCHAR ( 79 ), \n   ` o_dummy `            VARCHAR ( 10 )) \n    ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n    STORED   AS   TEXTFILE \n    LOCATION   '/tmp/tpch1g/orders/orders' ;  -- lineitem  CREATE   TABLE   IF   NOT   EXISTS   lineitem   ( \n   ` l_orderkey `            INT , \n   ` l_partkey `             INT , \n   ` l_suppkey `             INT , \n   ` l_linenumber `          INT , \n   ` l_quantity `            DECIMAL ( 15 , 2 ), \n   ` l_extendedprice `       DECIMAL ( 15 , 2 ), \n   ` l_discount `            DECIMAL ( 15 , 2 ), \n   ` l_tax `                 DECIMAL ( 15 , 2 ), \n   ` l_returnflag `          CHAR ( 1 ), \n   ` l_linestatus `          CHAR ( 1 ), \n   ` l_shipdate `            DATE , \n   ` l_commitdate `          DATE , \n   ` l_receiptdate `         DATE , \n   ` l_shipinstruct `        CHAR ( 25 ), \n   ` l_shipmode `            CHAR ( 10 ), \n   ` l_comment `             VARCHAR ( 44 ), \n   ` l_dummy `               VARCHAR ( 10 )) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   STORED   AS   TEXTFILE \n   LOCATION   '/tmp/tpch1g/lineitem/lineitem' ;",
            "title": "Create table and load data"
        },
        {
            "location": "/tutorial/tpch/#redshift",
            "text": "",
            "title": "Redshift"
        },
        {
            "location": "/tutorial/tpch/#create-tables_2",
            "text": "Use SQL query tools like  SQL Workbench/J  to connect with your Redshift Cluster.  First, Create a schema for testing create   schema   \"tpch1g\" ;  \nThen Create empty tables. -- nation  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"nation\"   ( \n   \"n_nationkey\"    INT , \n   \"n_name\"         CHAR ( 25 ), \n   \"n_regionkey\"    INT , \n   \"n_comment\"      VARCHAR ( 152 ), \n   \"n_dummy\"        VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"n_nationkey\" ));  -- region  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"region\"   ( \n   \"r_regionkey\"    INT , \n   \"r_name\"         CHAR ( 25 ), \n   \"r_comment\"      VARCHAR ( 152 ), \n   \"r_dummy\"        VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"r_regionkey\" ));  -- supplier  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"supplier\"   ( \n   \"s_suppkey\"       INT , \n   \"s_name\"          CHAR ( 25 ), \n   \"s_address\"       VARCHAR ( 40 ), \n   \"s_nationkey\"     INT , \n   \"s_phone\"         CHAR ( 15 ), \n   \"s_acctbal\"       DECIMAL ( 15 , 2 ), \n   \"s_comment\"       VARCHAR ( 101 ), \n   \"s_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"s_suppkey\" ));  -- customer  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"customer\"   ( \n   \"c_custkey\"       INT , \n   \"c_name\"          VARCHAR ( 25 ), \n   \"c_address\"       VARCHAR ( 40 ), \n   \"c_nationkey\"     INT , \n   \"c_phone\"         CHAR ( 15 ), \n   \"c_acctbal\"       DECIMAL ( 15 , 2 ), \n   \"c_mktsegment\"    CHAR ( 10 ), \n   \"c_comment\"       VARCHAR ( 117 ), \n   \"c_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"c_custkey\" ));  -- part  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"part\"   ( \n   \"p_partkey\"       INT , \n   \"p_name\"          VARCHAR ( 55 ), \n   \"p_mfgr\"          CHAR ( 25 ), \n   \"p_brand\"         CHAR ( 10 ), \n   \"p_type\"          VARCHAR ( 25 ), \n   \"p_size\"          INT , \n   \"p_container\"     CHAR ( 10 ), \n   \"p_retailprice\"   DECIMAL ( 15 , 2 )   , \n   \"p_comment\"       VARCHAR ( 23 )   , \n   \"p_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"p_partkey\" ));  -- partsupp  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"partsupp\"   ( \n   \"ps_partkey\"       INT , \n   \"ps_suppkey\"       INT , \n   \"ps_availqty\"      INT , \n   \"ps_supplycost\"    DECIMAL ( 15 , 2 ), \n   \"ps_comment\"       VARCHAR ( 199 ), \n   \"ps_dummy\"         VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"ps_partkey\" ));  -- orders  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"orders\"   ( \n   \"o_orderkey\"         INT , \n   \"o_custkey\"          INT , \n   \"o_orderstatus\"      CHAR ( 1 ), \n   \"o_totalprice\"       DECIMAL ( 15 , 2 ), \n   \"o_orderdate\"        DATE , \n   \"o_orderpriority\"    CHAR ( 15 ), \n   \"o_clerk\"            CHAR ( 15 ), \n   \"o_shippriority\"     INT , \n   \"o_comment\"          VARCHAR ( 79 ), \n   \"o_dummy\"            VARCHAR ( 10 ), \n   PRIMARY   KEY   ( \"o_orderkey\" ));  -- lineitem  CREATE   TABLE   IF   NOT   EXISTS   \"tpch1g\" . \"lineitem\" ( \n   \"l_orderkey\"            INT , \n   \"l_partkey\"             INT , \n   \"l_suppkey\"             INT , \n   \"l_linenumber\"          INT , \n   \"l_quantity\"            DECIMAL ( 15 , 2 ), \n   \"l_extendedprice\"       DECIMAL ( 15 , 2 ), \n   \"l_discount\"            DECIMAL ( 15 , 2 ), \n   \"l_tax\"                 DECIMAL ( 15 , 2 ), \n   \"l_returnflag\"          CHAR ( 1 ), \n   \"l_linestatus\"          CHAR ( 1 ), \n   \"l_shipdate\"            DATE , \n   \"l_commitdate\"          DATE , \n   \"l_receiptdate\"         DATE , \n   \"l_shipinstruct\"        CHAR ( 25 ), \n   \"l_shipmode\"            CHAR ( 10 ), \n   \"l_comment\"             VARCHAR ( 44 ), \n   \"l_dummy\"               VARCHAR ( 10 ));",
            "title": "Create tables"
        },
        {
            "location": "/tutorial/tpch/#load-data",
            "text": "For Redshift, we use Java method to help inserting our data into Redshift tables.  schema  is the Redshift schema you create your TPC-H tables\nand  table  is the table name of TPCH-table, such as  nation ,  region .  conn  is the Connection class you get from connecting your Redshift cluster using Redshift JDBC driver.\nSuppose your work directory is  /home/username/workspace  and the tpch1g data is stored in  /home/username/workspace/tpch1g . static   void   loadRedshiftData ( String   schema ,   String   table ,   Connection   conn ) \n       throws   IOException   { \n     Statement   stmt   =   conn . createStatement (); \n     String   concat   =   \"\" ; \n     File   file   = \n         new   File ( String . format ( \"/home/username/workspace/tpch1g/%s/%s.tbl\" ,   table ,   table )); \n     ResultSet   columnMeta   = \n         stmt . execute ( \n             String . format ( \n                 \"select data_type, ordinal_position from INFORMATION_SCHEMA.COLUMNS where table_name='%s' and table_schema='%s'\" , \n                 table ,   schema )); \n     List < Boolean >   quotedNeeded   =   new   ArrayList <>(); \n     for   ( int   i   =   0 ;   i   <   columnMeta . getRowCount ();   i ++)   { \n       quotedNeeded . add ( true ); \n     } \n     while   ( columnMeta . next ())   { \n       String   columnType   =   columnMeta . getString ( 1 ); \n       int   columnIndex   =   columnMeta . getInt ( 2 ); \n       if   ( columnType . equals ( \"integer\" )   ||   columnType . equals ( \"numeric\" ))   { \n         quotedNeeded . set ( columnIndex   -   1 ,   false ); \n       } \n     } \n\n     String   content   =   Files . toString ( file ,   Charsets . UTF_8 ); \n     for   ( String   row   :   content . split ( \"\\n\" ))   { \n       String []   values   =   row . split ( \"\\\\|\" ); \n       row   =   \"\" ; \n       for   ( int   i   =   0 ;   i   <   values . length   -   1 ;   i ++)   { \n         if   ( quotedNeeded . get ( i ))   { \n           row   =   row   +   getQuoted ( values [ i ])   +   \",\" ; \n         }   else   { \n           row   =   row   +   values [ i ]   +   \",\" ; \n         } \n       } \n       row   =   row   +   \"''\" ; \n       if   ( concat . equals ( \"\" ))   { \n         concat   =   concat   +   \"(\"   +   row   +   \")\" ; \n       }   else   concat   =   concat   +   \",\"   +   \"(\"   +   row   +   \")\" ; \n     } \n     stmt . execute ( String . format ( \"insert into \\\"%s\\\".\\\"%s\\\" values %s\" ,   schema ,   table ,   concat )); \n   }",
            "title": "Load Data"
        },
        {
            "location": "/tutorial/tpch/#cloudera-impala",
            "text": "",
            "title": "Cloudera Impala"
        },
        {
            "location": "/tutorial/tpch/#put-data-to-hdfs_1",
            "text": "Use following commands to put data into HDFS. Suppose the tpch1g data is stored in  /home/username/workspace/tpch1g  and you hope to put your data in  /tmp/tpch1g  in HDFS. $ hdfs dfs -mkdir -p /tmp/tpch1g/region\n$ hdfs dfs -mkdir -p /tmp/tpch1g/nation\n$ hdfs dfs -mkdir -p /tmp/tpch1g/customer\n$ hdfs dfs -mkdir -p /tmp/tpch1g/supplier\n$ hdfs dfs -mkdir -p /tmp/tpch1g/part\n$ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp\n$ hdfs dfs -mkdir -p /tmp/tpch1g/orders\n$ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem\n$ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl       /tmp/tpch1g/region\n$ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl       /tmp/tpch1g/nation\n$ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl   /tmp/tpch1g/customer\n$ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl   /tmp/tpch1g/supplier\n$ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl           /tmp/tpch1g/part\n$ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl   /tmp/tpch1g/partsupp\n$ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl       /tmp/tpch1g/orders\n$ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl   /tmp/tpch1g/lineitem  If you encounter write permission problem in the next step when creating tables, you can use command $ hdfs dfs -chmod -R  777  /tmp/tpch1g \nto give full access to your directory.",
            "title": "Put data to HDFS"
        },
        {
            "location": "/tutorial/tpch/#create-tables-and-load-data",
            "text": "Connect to Impala.  $ impala-shell  Create a schema for testing.  create schema `tpch1g`;  Create tables and load data. Simply copy and paste the following table definition statements into the Impala shell.  -- nation  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` nation `   ( \n   ` n_nationkey `    INT , \n   ` n_name `         STRING , \n   ` n_regionkey `    INT , \n   ` n_comment `      STRING , \n   ` n_dummy `        STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/nation' ;  -- region  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` region `   ( \n   ` r_regionkey `    INT , \n   ` r_name `         STRING , \n   ` r_comment `      STRING , \n   ` r_dummy `        STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/region' ;  -- supplier  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` supplier `   ( \n   ` s_suppkey `       INT , \n   ` s_name `          STRING , \n   ` s_address `       STRING , \n   ` s_nationkey `     INT , \n   ` s_phone `         STRING , \n   ` s_acctbal `       DECIMAL ( 15 , 2 ), \n   ` s_comment `       STRING , \n   ` s_dummy `         STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/supplier' ;  -- customer  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` customer `   ( \n   ` c_custkey `       INT , \n   ` c_name `          STRING , \n   ` c_address `       STRING , \n   ` c_nationkey `     INT , \n   ` c_phone `         STRING , \n   ` c_acctbal `       DECIMAL ( 15 , 2 ), \n   ` c_mktsegment `    STRING , \n   ` c_comment `       STRING , \n   ` c_dummy `         STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/customer' ;  -- part  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` part `   ( \n   ` p_partkey `       INT , \n   ` p_name `          STRING , \n   ` p_mfgr `          STRING , \n   ` p_brand `         STRING , \n   ` p_type `          STRING , \n   ` p_size `          INT , \n   ` p_container `     STRING , \n   ` p_retailprice `   DECIMAL ( 15 , 2 )   , \n   ` p_comment `       STRING   , \n   ` p_dummy `         STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/part' ;  -- partsupp  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` partsupp `   ( \n   ` ps_partkey `       INT , \n   ` ps_suppkey `       INT , \n   ` ps_availqty `      INT , \n   ` ps_supplycost `    DECIMAL ( 15 , 2 ), \n   ` ps_comment `       STRING , \n   ` ps_dummy `         STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/partsupp' ;  -- orders  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` orders `   ( \n   ` o_orderkey `         INT , \n   ` o_custkey `          INT , \n   ` o_orderstatus `      STRING , \n   ` o_totalprice `       DECIMAL ( 15 , 2 ), \n   ` o_orderdate `        TIMESTAMP , \n   ` o_orderpriority `    STRING , \n   ` o_clerk `            STRING , \n   ` o_shippriority `     INT , \n   ` o_comment `          STRING , \n   ` o_dummy `            STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/orders' ;  -- lineitem  CREATE   EXTERNAL   TABLE   IF   NOT   EXISTS   ` tpch1g ` . ` lineitem ` ( \n   ` l_orderkey `            INT , \n   ` l_partkey `             INT , \n   ` l_suppkey `             INT , \n   ` l_linenumber `          INT , \n   ` l_quantity `            DECIMAL ( 15 , 2 ), \n   ` l_extendedprice `       DECIMAL ( 15 , 2 ), \n   ` l_discount `            DECIMAL ( 15 , 2 ), \n   ` l_tax `                 DECIMAL ( 15 , 2 ), \n   ` l_returnflag `          STRING , \n   ` l_linestatus `          STRING , \n   ` l_shipdate `            TIMESTAMP , \n   ` l_commitdate `          TIMESTAMP , \n   ` l_receiptdate `         TIMESTAMP , \n   ` l_shipinstruct `        STRING , \n   ` l_shipmode `            STRING , \n   ` l_comment `             STRING , \n   ` l_dummy `               STRING ) \n   ROW   FORMAT   DELIMITED   FIELDS   TERMINATED   BY   '|' \n   LOCATION   '/tmp/tpch1g/lineitem' ;",
            "title": "Create tables and load data"
        },
        {
            "location": "/reference/supported_queries/",
            "text": "VerdictDB Queries\n\u00b6\n\n\nSupported queries\n\u00b6\n\n\nOverview\n\u00b6\n\n\nVerdict brings significant speedups for many important analytic queries. Before providing a detailed presentation on the queries Verdict can bring speedups, the following summary provides a quick overview.\n\n\n\n\n\n\nVerdict brings speedups for the queries including \naggregate functions\n.\n\n\n\n\n\n\nVerdict can speedup most common aggregate functions\n\n\n\n\n\n\nThe only known exceptions are extreme statistics: \nmin\n and \nmax\n.\n\n\n\n\n\n\nVerdict can speedup the queries including joins of multiple tables and subqueries.\n\n\n\n\n\n\nFor relatively simple queries (whose depth is no more than three), Verdict mostly brings speedups.\n\n\n\n\n\n\nFor deeper, complex queries (such as aggregations over aggregations), speedups more depend on Verdict's sample planner.\n\n\n\n\n\n\nThe cost of the worst cases will simply be equivalent to running the original queries.\n\n\n\n\n\n\nQuerying database metadata\n\u00b6\n\n\nshow databases;\n\n\n\n\nDisplays database names. When Verdict runs on Spark, it displays the tables accessible through \nHiveContext\n.\n\n\nuse database_name;\n\n\n\n\nSet the default database to \ndatabase-name\n.\n\n\nshow tables [in database_name];\n\n\n\n\nDisplays the tables in \ndatabase-name\n. If the database name is not specified, the tables in the default database are listed. If \ndatabase-name\n is not specified and the default database is not chosen, Verdict throws an error.\n\n\ndescribe [database_name].table_name;\n\n\n\n\nDisplays the column names and their types.\n\n\nshow samples [for database_name];\n\n\n\n\nDisplays the samples created for the tables in the \ndatabase-name\n. If \ndatabase-name\n is not specified and the database name is not specified, the samples created for the default database are specified. If the default database is not chosen, Verdict throws an error.\n\n\nCreating samples\n\u00b6\n\n\ncreate [XX%] sample of [database_name.]table_name;\n\n\n\n\nCreates a set of samples for the specified table. \nThis is the recommended way of creating sample tables.\n Verdict analyzes the statistics of the table and automatically creates desired samples. If the sampling probability is omitted, 1% samples are created by default.\n\n\nCurrently, Verdict creates three types of samples using the following rule:\n\n\n\n\n\n\nA uniform random sample\n\n\n\n\n\n\nStratified samples for low-cardinality columns (distinct-count of a column <= 1% of the total number of tuples).\n\n\n\n\n\n\nUniverse samples for high-cardinality columns (distinct-count of a column > 1% of the total number of tuples).\n\n\n\n\n\n\nFind more details on each sample type below.\n\n\ncreate [XX%] uniform sample of [database_name.]table_name;\n\n\n\n\nCreates XX% (1% by default) uniform random sample of the specified table.\n\n\nUniform random samples\n are useful for estimating some basic statistics, such as the number of tuples in a table, average of some expressions, etc., especially when there are no \ngroup-by\n clause.\n\n\ncreate [XX%] stratified sample of [database_name.]table_name on column_name;\n\n\n\n\nCreates XX% (1% by default) stratified sample of the specified table.\n\n\nStratified samples\n ensures that no attribute values are dropped for the column(s) on which the stratified samples are built on. This implies that, in the result set of \nselect group-name, count(*) from t group by group-name\n, every \ngroup-name\n exists even if Verdict runs the query on a sample. Stratified samples can be very useful when users have some knowledge on what columns will appear frequently in the \ngroup-by\n clause or in the \nwhere\n clause. For example, stratified samples can be very useful for streaming data where each record contains a timestamp. Building a stratified sample on the timestamp will ensure rare events are never dropped in the sampling process. Note that the sampling probabilities for tuples are not uniform in stratified sampling. However, Verdict automatically considers them and produce correct answers.\n\n\ncreate [XX%] universe sample of [database_name.]table_name on column_name;\n\n\n\n\nCreates XX% (1% by default) universe sample of the specified table.\n\n\nUniverse samples\n are hashing-based sampling. Verdict uses hash functions available in a database system it works with. Universe samples are used for estimating \ndistinct-count\n of high-cardinality columns. The theory behind using universe samples for \ndistinct-count\n is closely related to the \nHyperLogLog algorithm <https://en.wikipedia.org/wiki/HyperLogLog>\n_. Different from HyperLogLog, however, Verdict's approach uses a sample; thus, it is significantly faster than HyperLogLog or any similar approaches that scan the entire data. Also, universe samples are useful when a query includes joins. The equi-joins of two universe samples (of different tables) built on the join key preserves the cardinality very well; thus, it can produce accurate answers compared to joining two uniform or stratified samples.\n\n\nAnalyzing data: query structure\n\u00b6\n\n\nVerdict processes the standard SQL query in the following form::\n\n\nselect expr1, expr2, ...\nfrom table_source1, table_source2, ...\n[where conditions]\n[group by expr1, ...]\n[order by expr1]\n[limit n;]\n\n\n\n\n\nFind more details on the supported statements below.\n\n\nAnalyzing data: aggregate functions\n\u00b6\n\n\nSupported\n aggregate functions\n\u00b6\n\n\nVerdict brings speedups for the following aggregate functions or combinations of them:\n\n\n\n\n\n\n\n\nAggregate function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncount(*)\n\n\nCounts the number of tuples that satisfy selection conditions in the where clause (if any)\n\n\n\n\n\n\n\n\n\n\nsum(col-name)\n\n\nComputes the summation of the \nnon-null\n attribute values in the \"col-name\" column.\n\n\n\n\n\n\n\n\n\n\navg(col-name)\n\n\nComputes the avreage of the \nnon-null\n attribute values in the \"col-name\" column.\n\n\n\n\n\n\n\n\n\n\ncount(distinct col-name)\n\n\nComputes the number of distinct attributes in the \"col-name\" column; only one column can be specified.\n\n\n\n\n\n\n\n\nFuture supported\n aggregate functions\n\u00b6\n\n\nVerdict will be extended to support the following aggregate functions in the future:\n\n\n\n\n\n\n\n\nAggregate function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvar_pop(col-name)\n\n\npopulation variance\n\n\n\n\n\n\n\n\n\n\nvar_samp(col-name)\n\n\nsample variance\n\n\n\n\n\n\n\n\n\n\nstddev_pop(col-name)\n\n\npopulation standard deviation\n\n\n\n\n\n\n\n\n\n\nstddev_samp(col-name)\n\n\nsample standard deviation\n\n\n\n\n\n\n\n\n\n\ncovar_pop(col1, col2)\n\n\npopulation covariance\n\n\n\n\n\n\n\n\n\n\ncovar_samp(col1, col2)\n\n\nsample covariance\n\n\n\n\n\n\n\n\n\n\ncorr(col1, col2)\n\n\nPearson correlation coefficient\n\n\n\n\n\n\n\n\n\n\npercentile(col1, p)\n\n\np should be within 0.01 and 0.99 for reliable results\n\n\n\n\n\n\n\n\nNo-speedup\n aggregate functions\n\u00b6\n\n\nVerdict does not bring speedups (even in the future) for the following extreme statistics:\n\n\n\n\n\n\n\n\nAggregate function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmin(col-name)\n\n\nMin of the attribute values in the \"col-name\" column\n\n\n\n\n\n\n\n\n\n\nmax(col-name)\n\n\nMax of the attribute values in the \"col-name\" column\n\n\n\n\n\n\n\n\nIf a query includes these no-speedup aggregate function(s), Verdict uses the original tables (instead of the sample tables) for processing those queries.\n\n\nAnalyzing data: other functions\n\u00b6\n\n\nIn general, every (non-aggregate) function that is provided by existing database systems can be processed by Verdict (since Verdict will simply pass those functions to those databases). Please inform us if you want certain functions to be included. We will quickly add them.\n\n\nMathematical functions\n\u00b6\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nround(double a)\n      -\n\n\n\n\n\n\n\n\n\n\nfloor(double a)\n      -\n\n\n\n\n\n\n\n\n\n\nceil(double a)\n      -\n\n\n\n\n\n\n\n\n\n\nexp(double a)\n      -\n\n\n\n\n\n\n\n\n\n\nln(double a)\n\n\na natural logarithm\n\n\n\n\n\n\n\n\n\n\nlog10(double a)\n\n\nlog with base 10\n\n\n\n\n\n\n\n\n\n\nlog2(double a)\n\n\nlog with base 2\n\n\n\n\n\n\n\n\n\n\nsin(double a)\n      -\n\n\n\n\n\n\n\n\n\n\ncos(double a)\n      -\n\n\n\n\n\n\n\n\n\n\ntan(double a)\n      -\n\n\n\n\n\n\n\n\n\n\nsign(double a)\n\n\nReturns the sign of a as '1.0' (if a is positive) or '-1.0' (if a is negative), '0.0' otherwise\n\n\n\n\n\n\n\n\n\n\npmod(int a, int b)\n\n\na mod b; supported for Hive and Spark; See \nthis <page https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF>\n_ for more information.\n\n\n\n\n\n\n\n\n\n\na % b\n\n\na mod b\n\n\n\n\n\n\n\n\n\n\nrand(int seed)\n\n\nrandom number between 0 and 1\n\n\n\n\n\n\n\n\n\n\nabs(double a), abs(int a)\n\n\nan absolute value\n\n\n\n\n\n\n\n\n\n\nsqrt(double a)\n      -\n\n\n\n\n\n\n\n\nString operators\n\u00b6\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconv(int num, int from_base, int to_base), conv(string num, int from_base, int to_base)\n\n\nConverts a number from a given base to another; supported for Hive and Spark\n\n\n\n\n\n\n\n\n\n\nsubstr(string a, int start, int len)\n\n\nReturns the portion of the string starting at a specified point with a specified maximum length.\n\n\n\n\n\n\n\n\nOther functions\n\u00b6\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfnv_hash(expr)\n\n\nReturns a consistent 64-bit value derived from the input argument; supported for Impala; See \nthis page <https://www.cloudera.com/documentation/enterprise/5-8-x/topics/impala_math_functions.html>\n_ for more information.\n\n\n\n\n\n\n\n\n\n\nmd5(expr)\n\n\nCalculates an MD5 128-bit checksum for the string or binary; supported for Hive and Spark\n\n\n\n\n\n\n\n\n\n\ncrc32(expr)\n\n\nComputes a cyclic redundancy check value for string or binary argument and returns bigint value; supported for Hive and Spark\n\n\n\n\n\n\n\n\nAnalyzing data: table sources, filtering predicates, etc.\n\u00b6\n\n\nTable sources\n\u00b6\n\n\nYou can use a single base table, equi-joined tables, or derived tables in the from clause. Verdict's sample planner automatically finds the best set of sample tables to process your queries. However, if samples must not be used for processing your queries (due to unguaranteed accuracy), Verdict will use the original tables.\n\n\nVerdict's sample planner is rather involved, so we will make a separate document for its description.\n\n\nNote\n: Verdict's query parser currently processes only inner joins, but it will be extended to process left outer and right outer joins.\n\n\nFiltering predicates (i.e., in the where clause)\n\u00b6\n\n\n\n\n\n\n\n\nPredicate\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\np1 AND p2\n\n\nlogical and of two predicates, p1 and p2\n\n\n\n\n\n\n\n\n\n\np1 OR p2\n\n\nlogical or of two predicates, p1 and p2\n\n\n\n\n\n\n\n\n\n\nexpr1 COMP expr2\n\n\ncomparison of two expressions, expr1 and expr2, using the comparison operator, COMP. The available comparison operators are =, >, <, <=, >=, <>, !=, !>, !<, <=, >=, <, >, !>, !<\n\n\n\n\n\n\n\n\n\n\nexpr COMP (subquery)\n\n\ncomparison of the value of expr and the value of subquery. The subquery must return a single row and a single column\n\n\n\n\n\n\n\n\n\n\nexpr1 NOT? BETWEEN expr2 AND expr3\n\n\nreturns true if the value of expr1 is between the value of expr2 and the value of expr3.\n\n\n\n\n\n\n\n\n\n\nexpr1 NOT? LIKE expr2\n\n\ntext pattern search using wild cards. See \nthis page <https://www.w3schools.com/sql/sql_like.asp>\n_ for more information.\n\n\n\n\n\n\n\n\n\n\nexpr IS NOT? NULL\n\n\ntest if the value of the expression is null.\n\n\n\n\n\n\n\n\nDropping samples\n\u00b6\n\n\n(delete | drop) [XX%] samples of [database-name.]table-name;\n\n\n\n\nDrop all the samples created for the specified table. The sampling ratio is 1% is not specified explicitly.\n\n\n(delete | drop) [XX%] uniform samples of [database-name.]table-name;\n\n\n\n\nDrop the uniform random sample created for the specified table. The sampling ratio is 1% is not specified explicitly.\n\n\n(delete | drop) [XX%] stratified samples of [database-name.]table-name on column-name;\n\n\n\n\nDrop the stratified sample created for the specified table. The sampling ratio is 1% is not specified explicitly.\n\n\n(delete | drop) [XX%] universe samples of [database-name.]table-name on column-name;\n\n\n\n\nDrop the universe sample created for the specified table. The sampling ratio is 1% is not specified explicitly.\n\n\n(Later, Yongjoo will update the content)",
            "title": "VerdictDB Queries"
        },
        {
            "location": "/reference/supported_queries/#verdictdb-queries",
            "text": "",
            "title": "VerdictDB Queries"
        },
        {
            "location": "/reference/supported_queries/#supported-queries",
            "text": "",
            "title": "Supported queries"
        },
        {
            "location": "/reference/supported_queries/#overview",
            "text": "Verdict brings significant speedups for many important analytic queries. Before providing a detailed presentation on the queries Verdict can bring speedups, the following summary provides a quick overview.    Verdict brings speedups for the queries including  aggregate functions .    Verdict can speedup most common aggregate functions    The only known exceptions are extreme statistics:  min  and  max .    Verdict can speedup the queries including joins of multiple tables and subqueries.    For relatively simple queries (whose depth is no more than three), Verdict mostly brings speedups.    For deeper, complex queries (such as aggregations over aggregations), speedups more depend on Verdict's sample planner.    The cost of the worst cases will simply be equivalent to running the original queries.",
            "title": "Overview"
        },
        {
            "location": "/reference/supported_queries/#querying-database-metadata",
            "text": "show databases;  Displays database names. When Verdict runs on Spark, it displays the tables accessible through  HiveContext .  use database_name;  Set the default database to  database-name .  show tables [in database_name];  Displays the tables in  database-name . If the database name is not specified, the tables in the default database are listed. If  database-name  is not specified and the default database is not chosen, Verdict throws an error.  describe [database_name].table_name;  Displays the column names and their types.  show samples [for database_name];  Displays the samples created for the tables in the  database-name . If  database-name  is not specified and the database name is not specified, the samples created for the default database are specified. If the default database is not chosen, Verdict throws an error.",
            "title": "Querying database metadata"
        },
        {
            "location": "/reference/supported_queries/#creating-samples",
            "text": "create [XX%] sample of [database_name.]table_name;  Creates a set of samples for the specified table.  This is the recommended way of creating sample tables.  Verdict analyzes the statistics of the table and automatically creates desired samples. If the sampling probability is omitted, 1% samples are created by default.  Currently, Verdict creates three types of samples using the following rule:    A uniform random sample    Stratified samples for low-cardinality columns (distinct-count of a column <= 1% of the total number of tuples).    Universe samples for high-cardinality columns (distinct-count of a column > 1% of the total number of tuples).    Find more details on each sample type below.  create [XX%] uniform sample of [database_name.]table_name;  Creates XX% (1% by default) uniform random sample of the specified table.  Uniform random samples  are useful for estimating some basic statistics, such as the number of tuples in a table, average of some expressions, etc., especially when there are no  group-by  clause.  create [XX%] stratified sample of [database_name.]table_name on column_name;  Creates XX% (1% by default) stratified sample of the specified table.  Stratified samples  ensures that no attribute values are dropped for the column(s) on which the stratified samples are built on. This implies that, in the result set of  select group-name, count(*) from t group by group-name , every  group-name  exists even if Verdict runs the query on a sample. Stratified samples can be very useful when users have some knowledge on what columns will appear frequently in the  group-by  clause or in the  where  clause. For example, stratified samples can be very useful for streaming data where each record contains a timestamp. Building a stratified sample on the timestamp will ensure rare events are never dropped in the sampling process. Note that the sampling probabilities for tuples are not uniform in stratified sampling. However, Verdict automatically considers them and produce correct answers.  create [XX%] universe sample of [database_name.]table_name on column_name;  Creates XX% (1% by default) universe sample of the specified table.  Universe samples  are hashing-based sampling. Verdict uses hash functions available in a database system it works with. Universe samples are used for estimating  distinct-count  of high-cardinality columns. The theory behind using universe samples for  distinct-count  is closely related to the  HyperLogLog algorithm <https://en.wikipedia.org/wiki/HyperLogLog> _. Different from HyperLogLog, however, Verdict's approach uses a sample; thus, it is significantly faster than HyperLogLog or any similar approaches that scan the entire data. Also, universe samples are useful when a query includes joins. The equi-joins of two universe samples (of different tables) built on the join key preserves the cardinality very well; thus, it can produce accurate answers compared to joining two uniform or stratified samples.",
            "title": "Creating samples"
        },
        {
            "location": "/reference/supported_queries/#analyzing-data-query-structure",
            "text": "Verdict processes the standard SQL query in the following form::  select expr1, expr2, ...\nfrom table_source1, table_source2, ...\n[where conditions]\n[group by expr1, ...]\n[order by expr1]\n[limit n;]  Find more details on the supported statements below.",
            "title": "Analyzing data: query structure"
        },
        {
            "location": "/reference/supported_queries/#analyzing-data-aggregate-functions",
            "text": "",
            "title": "Analyzing data: aggregate functions"
        },
        {
            "location": "/reference/supported_queries/#supported-aggregate-functions",
            "text": "Verdict brings speedups for the following aggregate functions or combinations of them:     Aggregate function  Description      count(*)  Counts the number of tuples that satisfy selection conditions in the where clause (if any)      sum(col-name)  Computes the summation of the  non-null  attribute values in the \"col-name\" column.      avg(col-name)  Computes the avreage of the  non-null  attribute values in the \"col-name\" column.      count(distinct col-name)  Computes the number of distinct attributes in the \"col-name\" column; only one column can be specified.",
            "title": "Supported aggregate functions"
        },
        {
            "location": "/reference/supported_queries/#future-supported-aggregate-functions",
            "text": "Verdict will be extended to support the following aggregate functions in the future:     Aggregate function  Description      var_pop(col-name)  population variance      var_samp(col-name)  sample variance      stddev_pop(col-name)  population standard deviation      stddev_samp(col-name)  sample standard deviation      covar_pop(col1, col2)  population covariance      covar_samp(col1, col2)  sample covariance      corr(col1, col2)  Pearson correlation coefficient      percentile(col1, p)  p should be within 0.01 and 0.99 for reliable results",
            "title": "Future supported aggregate functions"
        },
        {
            "location": "/reference/supported_queries/#no-speedup-aggregate-functions",
            "text": "Verdict does not bring speedups (even in the future) for the following extreme statistics:     Aggregate function  Description      min(col-name)  Min of the attribute values in the \"col-name\" column      max(col-name)  Max of the attribute values in the \"col-name\" column     If a query includes these no-speedup aggregate function(s), Verdict uses the original tables (instead of the sample tables) for processing those queries.",
            "title": "No-speedup aggregate functions"
        },
        {
            "location": "/reference/supported_queries/#analyzing-data-other-functions",
            "text": "In general, every (non-aggregate) function that is provided by existing database systems can be processed by Verdict (since Verdict will simply pass those functions to those databases). Please inform us if you want certain functions to be included. We will quickly add them.",
            "title": "Analyzing data: other functions"
        },
        {
            "location": "/reference/supported_queries/#mathematical-functions",
            "text": "Function  Description      round(double a)\n      -      floor(double a)\n      -      ceil(double a)\n      -      exp(double a)\n      -      ln(double a)  a natural logarithm      log10(double a)  log with base 10      log2(double a)  log with base 2      sin(double a)\n      -      cos(double a)\n      -      tan(double a)\n      -      sign(double a)  Returns the sign of a as '1.0' (if a is positive) or '-1.0' (if a is negative), '0.0' otherwise      pmod(int a, int b)  a mod b; supported for Hive and Spark; See  this <page https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF> _ for more information.      a % b  a mod b      rand(int seed)  random number between 0 and 1      abs(double a), abs(int a)  an absolute value      sqrt(double a)\n      -",
            "title": "Mathematical functions"
        },
        {
            "location": "/reference/supported_queries/#string-operators",
            "text": "Function  Description      conv(int num, int from_base, int to_base), conv(string num, int from_base, int to_base)  Converts a number from a given base to another; supported for Hive and Spark      substr(string a, int start, int len)  Returns the portion of the string starting at a specified point with a specified maximum length.",
            "title": "String operators"
        },
        {
            "location": "/reference/supported_queries/#other-functions",
            "text": "Function  Description      fnv_hash(expr)  Returns a consistent 64-bit value derived from the input argument; supported for Impala; See  this page <https://www.cloudera.com/documentation/enterprise/5-8-x/topics/impala_math_functions.html> _ for more information.      md5(expr)  Calculates an MD5 128-bit checksum for the string or binary; supported for Hive and Spark      crc32(expr)  Computes a cyclic redundancy check value for string or binary argument and returns bigint value; supported for Hive and Spark",
            "title": "Other functions"
        },
        {
            "location": "/reference/supported_queries/#analyzing-data-table-sources-filtering-predicates-etc",
            "text": "",
            "title": "Analyzing data: table sources, filtering predicates, etc."
        },
        {
            "location": "/reference/supported_queries/#table-sources",
            "text": "You can use a single base table, equi-joined tables, or derived tables in the from clause. Verdict's sample planner automatically finds the best set of sample tables to process your queries. However, if samples must not be used for processing your queries (due to unguaranteed accuracy), Verdict will use the original tables.  Verdict's sample planner is rather involved, so we will make a separate document for its description.  Note : Verdict's query parser currently processes only inner joins, but it will be extended to process left outer and right outer joins.",
            "title": "Table sources"
        },
        {
            "location": "/reference/supported_queries/#filtering-predicates-ie-in-the-where-clause",
            "text": "Predicate  Description      p1 AND p2  logical and of two predicates, p1 and p2      p1 OR p2  logical or of two predicates, p1 and p2      expr1 COMP expr2  comparison of two expressions, expr1 and expr2, using the comparison operator, COMP. The available comparison operators are =, >, <, <=, >=, <>, !=, !>, !<, <=, >=, <, >, !>, !<      expr COMP (subquery)  comparison of the value of expr and the value of subquery. The subquery must return a single row and a single column      expr1 NOT? BETWEEN expr2 AND expr3  returns true if the value of expr1 is between the value of expr2 and the value of expr3.      expr1 NOT? LIKE expr2  text pattern search using wild cards. See  this page <https://www.w3schools.com/sql/sql_like.asp> _ for more information.      expr IS NOT? NULL  test if the value of the expression is null.",
            "title": "Filtering predicates (i.e., in the where clause)"
        },
        {
            "location": "/reference/supported_queries/#dropping-samples",
            "text": "(delete | drop) [XX%] samples of [database-name.]table-name;  Drop all the samples created for the specified table. The sampling ratio is 1% is not specified explicitly.  (delete | drop) [XX%] uniform samples of [database-name.]table-name;  Drop the uniform random sample created for the specified table. The sampling ratio is 1% is not specified explicitly.  (delete | drop) [XX%] stratified samples of [database-name.]table-name on column-name;  Drop the stratified sample created for the specified table. The sampling ratio is 1% is not specified explicitly.  (delete | drop) [XX%] universe samples of [database-name.]table-name on column-name;  Drop the universe sample created for the specified table. The sampling ratio is 1% is not specified explicitly.  (Later, Yongjoo will update the content)",
            "title": "Dropping samples"
        }
    ]
}